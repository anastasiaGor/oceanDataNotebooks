{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb7152a-adfe-4999-8ca0-77e6536009a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9032ac0-1107-4896-9181-de7ef2615615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:42433\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36532fb-c2ed-43b3-8bd0-7c1086407d8a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07708f7a-4d51-459b-ac79-fe856c93a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from scipy import ndimage\n",
    "import itertools\n",
    "import os\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bf9717e6-2e96-4ad4-bc6e-2490ad6ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux-5.10.133+-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf23faa3-d7e0-4a71-bf09-67a11edff645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1.post200\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d23fce9-db7f-4af5-926b-085a045bc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c3028-781a-46cd-a308-635207e8f62e",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931cafd-2d13-413d-9048-6db903c72e38",
   "metadata": {},
   "source": [
    "## Useful xArray functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2239000-2adb-4edb-952e-517537d33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_2d_maps(xr_data, h, w) :\n",
    "    return xr_data.isel(x_c=slice(None,w), y_c=slice(None,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f889bc9-1159-4e74-8219-5b8f99aca7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_bin_mask(xr_mask) :\n",
    "    erosion_structure_matrix = np.array([(0,0,1,0,0), (0,1,1,1,0), (1,1,1,1,1), (0,1,1,1,0), (0,0,1,0,0)])\n",
    "    np_array_mask = ndimage.binary_erosion(xr_mask, structure=erosion_structure_matrix)\n",
    "    return xr_mask.copy(data=np_array_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847fdf9-c703-42bd-a692-3670ca67fd11",
   "metadata": {},
   "source": [
    "## Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e087d1c-e06b-44e8-a01b-520a4b204f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset of 2D maps of surface temperature, salinity\"\"\"\n",
    "\n",
    "    def __init__(self, xarray_dataset, features_to_add_to_sample, auxiliary_features, height, width):\n",
    "        self.features_to_add_to_sample = features_to_add_to_sample\n",
    "        self.auxiliary_features = auxiliary_features\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.data = crop_2d_maps(xarray_dataset, self.height, self.width).load()\n",
    "        self.data_file_len = len(self.data.t)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_file_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            list_idx = idx.tolist()\n",
    "        else :\n",
    "            list_idx = idx\n",
    "        selected_time_frames = self.data.isel(t=list_idx) #still xArray object\n",
    "        \n",
    "        # create dictionary of a sample (a batch) containig different features in numpy format. \n",
    "        # This dictionary is an intermediate step, preparing xArray data for transform into pytorch tensors\n",
    "        sample = dict()\n",
    "        sample['mask'] = toTorchTensor(selected_time_frames['mask'].astype(bool))\n",
    "        sample['eroded_mask'] = toTorchTensor(erode_bin_mask(selected_time_frames['mask']))\n",
    "        \n",
    "        for feature in self.features_to_add_to_sample :\n",
    "            sample['mean_'+feature] = toTorchTensor(self.data['mean_'+feature])\n",
    "            sample['std_'+feature] = toTorchTensor(self.data['std_'+feature])\n",
    "            sample[feature] = toTorchTensor(selected_time_frames[feature])\n",
    "\n",
    "        for feature in self.auxiliary_features :\n",
    "            sample[feature] = toTorchTensor(selected_time_frames[feature])\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fc0589-8400-4760-a827-0d8f822359c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTorchTensor(xrArray):\n",
    "    transformed_data = torch.tensor(xrArray.values)\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1323fc6-fc0c-4cc5-942a-ea87e895525f",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af829e9f-7769-4b2b-aa56-ad2d13dfc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyLiDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, cloud_data_sets, data_geometry, features_to_add_to_sample, auxiliary_features, height, width, batch_size) :\n",
    "        super().__init__()\n",
    "        self.cloud_data_sets = cloud_data_sets\n",
    "        self.data_geometry = data_geometry\n",
    "        self.features_to_add_to_sample = features_to_add_to_sample\n",
    "        self.auxiliary_features = auxiliary_features\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.batch_size = batch_size\n",
    "        self.list_of_xr_datasets = [xr.Dataset() for i in range(len(self.cloud_data_sets))]\n",
    "        self.list_of_torch_datasets = [{} for i in range(len(self.cloud_data_sets))]\n",
    "        \n",
    "    #def prepare_data(self) :\n",
    "        # preparation of data: mean and std of the dataset (to avoid batch avg), normalization and nan filling\n",
    "        for i in range(len(self.cloud_data_sets)) :\n",
    "            # read file\n",
    "            PERSISTENT_BUCKET = os.environ['PERSISTENT_BUCKET'] \n",
    "            if (self.data_geometry =='2D') :\n",
    "                file_prefix = 'data'\n",
    "            if (self.data_geometry =='3D') :\n",
    "                file_prefix = 'data3D_'\n",
    "            xr_dataset = xr.open_zarr(f'{PERSISTENT_BUCKET}/'+file_prefix+str(i)+'.zarr', chunks='auto')\\\n",
    "            [self.features_to_add_to_sample + self.auxiliary_features + ['mask']]\n",
    "            for feature in self.features_to_add_to_sample :\n",
    "                # reapply mask (to avoid issues with nans written in netcdf files)\n",
    "                xr_dataset[feature] = xr_dataset[feature].where(xr_dataset.mask>0)\n",
    "                # compute mean, median and std for each level (since temperature/salinity may change a lot with the depth)\n",
    "                xr_dataset['mean_'+feature] = (xr_dataset[feature].mean(dim=['t', 'x_c', 'y_c']))\n",
    "                xr_dataset['std_'+feature] = (xr_dataset[feature].std(dim=['t', 'x_c', 'y_c']))\n",
    "                # fill nans with mean (doesn't the number to be fillted in matter since they will be masked, \n",
    "                # but they have to be filled with any numbers so that nans do not propagate everywhere) \n",
    "                xr_dataset[feature] = xr_dataset[feature].fillna(xr_dataset['mean_'+feature])\n",
    "            # save result in a list\n",
    "            self.list_of_xr_datasets[i] = xr_dataset\n",
    "            self.list_of_torch_datasets[i] = torchDataset(xr_dataset, self.features_to_add_to_sample, self.auxiliary_features, self.height, self.width)\n",
    "            \n",
    "    def setup(self, stage: str) :\n",
    "        if (stage == 'fit') :\n",
    "        # takes first 60% of time snapshots for training\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([torch.utils.data.Subset(dataset, \\\n",
    "                                                                                     indices=range(0,int(0.6*len(dataset)))) \\\n",
    "                                                                                     for dataset in self.list_of_torch_datasets])\n",
    "        # takes last 20% of time snapshots for validation (we keep a gap to have validation data decorrelated from trainig data)\n",
    "            self.val_dataset = torch.utils.data.ConcatDataset([torch.utils.data.Subset(dataset, \\\n",
    "                                                                                     indices=range(int(0.8*len(dataset)),len(dataset))) \\\n",
    "                                                                                     for dataset in self.list_of_torch_datasets])\n",
    "        # same for test\n",
    "        if (stage == 'test') :\n",
    "            self.test_datasets = [torch.utils.data.Subset(dataset, indices=range(int(0.8*len(dataset)),len(dataset))) \\\n",
    "                                                               for dataset in self.list_of_torch_datasets]\n",
    "            \n",
    "                \n",
    "    def train_dataloader(self) :\n",
    "        # create training dataloadder from train_dataset with shuffling with given batch size\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, \\\n",
    "                                           batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "    \n",
    "    def val_dataloader(self) :\n",
    "        # create training dataloadder from val_dataset without shuffling with the same batch size\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, drop_last=True, num_workers=0) \n",
    "    \n",
    "    def test_dataloader(self) :\n",
    "        # create a LIST of dataloaders (a dataloader for each dataset) - to enable diagnostics in each region/season individually \n",
    "        # batch size is equal to the dataset length, i.e. there is ONLY 1 batch with all dataset inside (can be better since there is no optimisation in testing)\n",
    "        return [torch.utils.data.DataLoader(dataset, batch_size=len(dataset), drop_last=True, num_workers=0) for dataset in self.test_datasets]\n",
    "    \n",
    "    def teardown(self, stage : str) :\n",
    "        if (stage == 'fit') :\n",
    "            # clean train and val datasets to free memory\n",
    "            del self.train_dataset, self.val_dataset\n",
    "        # if (stage == 'test') :\n",
    "        #     del self.test_datasets   \n",
    "        # if (stage == 'predict') :\n",
    "        #     del self.test_datasets   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f231e-97bb-4509-8cf1-0858d533a8b4",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ad404-3303-48e0-8c58-faae3f1a7f62",
   "metadata": {},
   "source": [
    "### Differences and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c04af39e-6d7d-4e0f-9ef8-caa0c398358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_diffs(dataArray) :\n",
    "    if len(dataArray.shape) == 5 : #5d data cube\n",
    "        batch_len, nb_of_levels, nb_of_channels, output_h, output_w = dataArray.shape\n",
    "        flatten_data = dataArray.flatten(start_dim=0, end_dim=2)[:,None,:,:]\n",
    "    if len(dataArray.shape) == 4 : # 1 channel (or 1 level)\n",
    "        batch_len, nb_of_channels, width, height = dataArray.shape\n",
    "        flatten_data = dataArray.flatten(start_dim=0, end_dim=1)[:,None,:,:]\n",
    "    if len(dataArray.shape) == 3 : # 1 channel\n",
    "        batch_len, width, height = dataArray.shape\n",
    "        flatten_data = dataArray[:,None,:,:]\n",
    "    weights = torch.zeros(2,1,3,3).to(dataArray.device) # 2 channels : 1 channel for x-difference, other for y-differences\n",
    "    weights[0,0,:,:] = torch.tensor([[0,0.,0],[-0.5,0.,0.5],[0,0.,0]]) #dx\n",
    "    weights[1,0,:,:] = torch.tensor([[0,-0.5,0],[0,0.,0],[0,0.5,0]])   #dy\n",
    "    res = torch.nn.functional.conv2d(flatten_data.float(), weights, \\\n",
    "                               bias=None, stride=1, padding='same', dilation=1, groups=1)\n",
    "    if len(dataArray.shape) == 5 :\n",
    "        res_dx = res[:,0,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_levels, nb_of_channels))\n",
    "        res_dy = res[:,1,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_levels, nb_of_channels))\n",
    "    if len(dataArray.shape) == 4 :\n",
    "        res_dx = res[:,0,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_channels))\n",
    "        res_dy = res[:,1,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_channels))\n",
    "    if len(dataArray.shape) == 3 :\n",
    "        res_dx = res[:,0,1:-1,1:-1]\n",
    "        res_dy = res[:,1,1:-1,1:-1]\n",
    "    return res_dx, res_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "839f508e-157f-4565-a211-66930a230b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diffs_sqr_2d_map(dataArray) :\n",
    "    res_dx, res_dy = central_diffs(dataArray)\n",
    "    res = torch.pow(res_dx,2) + torch.pow(res_dy,2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d8d55-cf8b-4d24-970a-5e9682c42dc8",
   "metadata": {},
   "source": [
    "### Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33356835-e629-4d64-ae65-f426b40e89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pressure_grad(votemper_var, rho_ct_ct, dx, dy) :\n",
    "    g = 9.81\n",
    "    dz = torch.Tensor([ 1.0000261,  1.1568018,  1.3141856,  1.4721599,  1.6307058,\n",
    "        1.7898054,  1.94944  ,  2.1095896,  2.270236 ,  2.4313574,\n",
    "        2.5929375,  2.7549553,  2.917385 ,  3.0802155,  3.2434177,\n",
    "        3.4069748,  3.5708656,  3.7350693,  3.899559 ,  4.0643234,\n",
    "        4.229328 ,  4.394562 ,  4.5600014,  4.7256126,  4.8913956,\n",
    "        5.0572968,  5.2233276,  5.38945  ,  5.555626 ,  5.7218704,\n",
    "        5.888115 ,  6.0543823,  6.2206116,  6.3868027,  6.5529404,\n",
    "        6.7189636,  6.884903 ,  7.0506744,  7.216324 ,  7.38179  ,\n",
    "        7.547043 ,  7.7120667,  7.876877 ,  8.041382 ,  8.205627 ,\n",
    "        8.369553 ,  8.533157 ,  8.696396 ,  8.859283 ,  9.021744 ,\n",
    "        9.183807 ,  9.345459 ,  9.506622 ,  9.667328 ,  9.827545 ,\n",
    "        9.987244 , 10.146393 , 10.305023 , 10.463043 , 10.620514 ,\n",
    "       10.777374 , 10.933563 , 11.089172 , 11.24411  , 11.398346 ,\n",
    "       11.55191  , 11.704773 , 11.856903 , 12.008301 , 12.158936 ,\n",
    "       12.308777 , 12.457916 , 12.606201 , 12.753632 , 12.900391 ,\n",
    "       13.046143 , 13.191162 , 13.335266 , 13.478516 , 13.62085  ,\n",
    "       13.762329 , 13.902893 , 14.042603 , 14.181274 , 14.319092 ,\n",
    "       14.455872 , 14.591858 , 14.726746 , 14.860718 , 14.993713 ,\n",
    "       15.125732 , 15.256775 , 15.38678  , 15.515808 , 15.64386  ,\n",
    "       15.770813 , 15.896851 , 16.02179  , 16.14569  , 16.268677 ,\n",
    "       16.390503 , 16.511353 , 16.631165 , 16.749939 , 16.867676 ,\n",
    "       16.984314 ]).to(votemper_var.device)\n",
    "    delta_rho = 0.5*votemper_var*rho_ct_ct\n",
    "    dx_rho, dy_rho = central_diffs(delta_rho)\n",
    "    dx_rho = dx_rho[:,:-1,:,:]/dx[:,None,1:-1,1:-1]\n",
    "    dy_rho = dy_rho[:,:-1,:,:]/dy[:,None,1:-1,1:-1]\n",
    "    dx_p = torch.cumsum(dx_rho*g*dz[None,:,None,None], axis=1)   \n",
    "    dy_p = torch.cumsum(dy_rho*g*dz[None,:,None,None], axis=1)\n",
    "    return [dx_p, dy_p, torch.sqrt(torch.pow(dx_p,2)+torch.pow(dy_p,2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d0da6-df75-45ce-8a7f-1ab16ef602d2",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "664101b0-a44b-4b06-97c1-eeac3ca48094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tensor_metrics_with_mask(data_geometry, metrics, mask, truth, model_output, reduction='mean') :\n",
    "    if (data_geometry == '2D') :\n",
    "        if (len(model_output.shape) == 4) : # 4D tensor with C features\n",
    "            batch_len, nb_of_channels, output_h, output_w = model_output.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_channels\n",
    "            mask = mask[:,None,:,:]\n",
    "        if (len(model_output.shape) == 3) : # 1 feature (1 channel)- 3D tensor\n",
    "            batch_len, output_h, output_w = model_output.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)\n",
    "            mask = mask\n",
    "    if (data_geometry == '3D') :\n",
    "        if (len(model_output.shape) == 5) : # full 5D tensor\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = model_output.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_levels*nb_of_channels\n",
    "            mask = mask[:,None,None,:,:]\n",
    "        if (len(model_output.shape) == 4) : # 1 feature (1 channel)\n",
    "            batch_len, nb_of_levels, output_h, output_w = model_output.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_levels\n",
    "            mask = mask[:,None,:,:]\n",
    "        if (len(model_output.shape) == 3) : # 1 feature (1 channel) and 1 level\n",
    "            batch_len, output_h, output_w = model_output.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)\n",
    "            mask = mask[:,:,:]\n",
    "\n",
    "    if (reduction=='none') : \n",
    "        return metrics(model_output*mask, truth*mask, reduction='none')\n",
    "\n",
    "    total_metrics = metrics(model_output*mask, truth*mask, reduction='sum')\n",
    "    if (reduction=='mean') : \n",
    "        return (total_metrics/valid_mask_counts)\n",
    "    if (reduction=='sum') : \n",
    "        return (total_metrics)\n",
    "    if (reduction=='vertical') :\n",
    "        sum_over_each_layer = torch.sum(metrics(model_output*mask, truth*mask, reduction='none'), dim=(2,3))\n",
    "        valid_counts_each_layer = torch.count_nonzero(mask, dim=(1,2))\n",
    "        vertical_profile_of_each_sample = sum_over_each_layer/valid_counts_each_layer\n",
    "        return torch.mean(vertical_profile_of_each_sample, dim=0)\n",
    "    if (reduction=='horizontal') :\n",
    "        sum_over_depth_at_each_point = torch.sum(metrics(model_output*mask, truth*mask, reduction='none'), dim=1)\n",
    "        valid_counts = torch.count_nonzero(mask)\n",
    "        horizontal_error_of_each_sample = sum_over_depth_at_each_point/valid_counts\n",
    "        return torch.mean(horizontal_error_of_each_sample, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "49182112-838d-42f8-809c-5e171d7ac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_based_MSEloss(pred_sigma, target_sigma, rho_ct_ct, idx_level, mask, dx, dy) :\n",
    "    true_pres_grad_x, true_pres_grad_y, true_pres_grad_norm = get_pressure_grad(target_sigma, rho_ct_ct, dx, dy)\n",
    "    pred_pres_grad_x, pred_pres_grad_y, pred_pres_grad_norm = get_pressure_grad(pred_sigma, rho_ct_ct, dx, dy)\n",
    "\n",
    "    grad_x_loss = evaluate_tensor_metrics_with_mask('3D', torch.nn.functional.mse_loss, mask[:,1:-1,1:-1], \\\n",
    "                                                        pred_pres_grad_x[:,idx_level,:,:], true_pres_grad_x[:,idx_level,:,:], reduction='mean')\n",
    "    grad_y_loss = evaluate_tensor_metrics_with_mask('3D', torch.nn.functional.mse_loss, mask[:,1:-1,1:-1], \\\n",
    "                                                        pred_pres_grad_y[:,idx_level,:,:], true_pres_grad_y[:,idx_level,:,:], reduction='mean')\n",
    "    grad_loss = grad_x_loss+grad_y_loss\n",
    "    return grad_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d661d-3e8f-4f17-822b-cf71602c0055",
   "metadata": {},
   "source": [
    "### Tensor transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c3913f-cff1-4375-b586-bff5ef68a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_bords(tensor, nb_of_border_pix) :\n",
    "    if nb_of_border_pix is None :\n",
    "        return tensor\n",
    "    else :\n",
    "        if (len(tensor.shape) == 5) :\n",
    "            return tensor[:,:, :, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] \n",
    "        if (len(tensor.shape) == 4) :\n",
    "            return tensor[:,:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] \n",
    "        if (len(tensor.shape) == 3) :\n",
    "            return tensor[:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d2e8e7-78ce-443d-bca4-8fb182df26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_bords(tensor, nb_of_border_pix) :\n",
    "    if nb_of_border_pix is None :\n",
    "        return tensor\n",
    "    else :\n",
    "        if (len(tensor.shape) == 4) :\n",
    "            new_tensor = torch.empty((tensor.shape[0],tensor.shape[1], tensor.shape[2]+2*nb_of_border_pix, tensor.shape[3]+2*nb_of_border_pix)).\\\n",
    "            to(tensor.device)\n",
    "            new_tensor[:,:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] = tensor\n",
    "        if (len(tensor.shape) == 3) :\n",
    "            new_tensor = torch.empty((tensor.shape[0], tensor.shape[1]+2*nb_of_border_pix, tensor.shape[2]+2*nb_of_border_pix)).to(tensor.device)\n",
    "            new_tensor[:,nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] = tensor        \n",
    "        return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "16b6494a-3c08-4b94-afa3-b6e6b495040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(batch, features, nb_of_border_pix) :\n",
    "    # check if normalization is needed\n",
    "    for feature in features :\n",
    "        if feature.startswith('normalized_') :\n",
    "            not_normalized_feature_name = feature.replace(\"normalized_\", \"\")\n",
    "            batch['normalized_'+not_normalized_feature_name] = tensor_normalize(batch[not_normalized_feature_name], batch, not_normalized_feature_name)\n",
    "        if feature.startswith('filtered_') :\n",
    "            not_filt_feature_name = feature.replace(\"filtered_\", \"\")\n",
    "            batch['filtered_'+not_filt_feature_name] = filter_with_convolution(batch[not_filt_feature_name], convolution_kernel_size=3)\n",
    "    # stack features from sample into channel (create channel dimension in tensor)\n",
    "    stacked_channels = torch.stack([cut_bords(batch[key], nb_of_border_pix) for key in features])\n",
    "    if (len(stacked_channels.shape) == 4): # 2d data case -> 4D cubes \n",
    "        transform = torch.permute(stacked_channels, (1,0,2,3)).to(torch.float32) #shape [N,C,H,W]\n",
    "    if (len(stacked_channels.shape) == 5): # 3d data case -> 5d cubes\n",
    "        transform = torch.permute(stacked_channels, (1,2,0,3,4)).to(torch.float32) #shape [N,L,C,H,W]\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "85a96d77-1deb-4f02-8fff-9f59ce6b81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_restore_norm(tensor, batch, reference_feature) :\n",
    "    if (len(tensor.shape) == 3) :\n",
    "        std = batch['std_'+reference_feature][:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,None,None]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        std = batch['std_'+reference_feature][:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,None,None]\n",
    "    if (len(tensor.shape) == 5) :\n",
    "        std = batch['std_'+reference_feature][:,:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,:, None,None]\n",
    "    return tensor*std+mean\n",
    "\n",
    "def tensor_normalize(tensor, batch, reference_feature) :\n",
    "    if (len(tensor.shape) == 3) :\n",
    "        std = batch['std_'+reference_feature][:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,None,None]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        std = batch['std_'+reference_feature][:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,None,None]\n",
    "    if (len(tensor.shape) == 5) :\n",
    "        std = batch['std_'+reference_feature][:,:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,:, None,None]\n",
    "    return (tensor-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "51e3d393-0a09-4eee-9b6a-5c5ab76f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_with_convolution(tensor, convolution_kernel_size=3) :\n",
    "    if (len(tensor.shape) == 4) : #[N,L,H,W]\n",
    "        batch_len, nb_levels, height, width = tensor.shape\n",
    "        flatten_tensor = tensor.flatten(start_dim=0, end_dim=1)[:,None,:,:]\n",
    "    if (len(tensor.shape) == 3) : #[N,H,W]\n",
    "        flatten_tensor = tensor[:,None,:,:]\n",
    "        \n",
    "    weights = torch.ones(1,1,convolution_kernel_size,convolution_kernel_size).to(tensor.device) #matrix filled with ones for averaging\n",
    "    padding = torch.nn.ReplicationPad2d(convolution_kernel_size//2)  #pad borders with 1 row/column with the replicated values\n",
    "    padded_tensor= padding(flatten_tensor)\n",
    "    res = torch.nn.functional.conv2d(padded_tensor, weights, bias=None, stride=1, padding='valid', dilation=1, groups=1)\n",
    "    res = res[:,0,:,:]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        res = res.unflatten(dim=0, sizes=(batch_len, nb_levels))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0a560-2600-409a-be49-cfe767b7d50a",
   "metadata": {},
   "source": [
    "## PyTorch Lighning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d7744d06-1873-4f38-918b-030e009b6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericPyLiModule(pl.LightningModule):\n",
    "    def __init__(self, torch_model, input_features, output_features, output_units, loss, optimizer, learning_rate):\n",
    "        super().__init__()\n",
    "        self.torch_model = torch_model\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ## initialization of weights\n",
    "        #torch_model.weight.data = torch.Tensor([1.0])\n",
    "\n",
    "        #construct list of names of features to be predicted\n",
    "        self.list_of_features_to_predict=list()\n",
    "        for i, feature in enumerate(self.output_features) :\n",
    "            self.list_of_features_to_predict.append(feature)\n",
    "            if feature.startswith('normalized_') :\n",
    "                # if model output is a normalized feature then compute also the non-normalized feature (for the diagnostics)\n",
    "                not_normalized_feature = feature.replace(\"normalized_\", \"\")\n",
    "                self.list_of_features_to_predict.append(not_normalized_feature)\n",
    "                \n",
    "        self.data_geometry = self.torch_model.data_geometry\n",
    "    \n",
    "    def step(self, batch, batch_idx) :\n",
    "        x = transform_features(batch, self.input_features, self.torch_model.cut_border_pix_input)\n",
    "        y_true = transform_features(batch, self.output_features, self.torch_model.cut_border_pix_output)\n",
    "        mask = cut_bords(batch['eroded_mask'], self.torch_model.cut_border_pix_output)\n",
    "\n",
    "        if (self.output_units is None) :\n",
    "            y_model = self.torch_model(x)\n",
    "        else :\n",
    "            y_units = transform_features(batch, self.output_units, self.torch_model.cut_border_pix_output)\n",
    "            y_model = y_units*self.torch_model(x)\n",
    "            \n",
    "        first_layer_weights = list(self.torch_model.__dict__['_modules'].values())[0].weight # for logging\n",
    "        \n",
    "        if (self.loss=='pressure_based_MSEloss') :\n",
    "            if (self.data_geometry != '3D') :\n",
    "                print('ERROR: pressure based loss is available only for 3D data')\n",
    "                return\n",
    "            rho_ct_ct = cut_bords(batch['rho_ct_ct'], self.torch_model.cut_border_pix_output)\n",
    "            dx = cut_bords(batch['e1t'], self.torch_model.cut_border_pix_output)\n",
    "            dy = cut_bords(batch['e2t'], self.torch_model.cut_border_pix_output)\n",
    "            pred_sigma = y_model[:, :, 0, :, :]\n",
    "            target_sigma = y_true[:, :, 0, :, :]\n",
    "            idx_level=100\n",
    "            loss_pres = pressure_based_MSEloss(pred_sigma, target_sigma, rho_ct_ct, idx_level, mask, dx, dy)\n",
    "            loss_val = evaluate_tensor_metrics_with_mask(self.data_geometry, torch.nn.functional.mse_loss, mask, y_model, y_true)\n",
    "            loss_total = 1e6*loss_pres+loss_val\n",
    "            logs = dict({'loss_train' : loss_total,\n",
    "                           'loss_pressure' : loss_pres,\n",
    "                           'loss_val' : loss_val,\n",
    "                      'first_weight' : np.array(first_layer_weights.cpu().detach().numpy()).flat[0]})\n",
    "        else :\n",
    "            loss_val = evaluate_tensor_metrics_with_mask(self.data_geometry, self.loss, mask, y_model, y_true)  \n",
    "            logs= dict({'loss_train' : loss_val,\n",
    "                      'first_weight' : np.array(first_layer_weights.cpu().detach().numpy()).flat[0]})   \n",
    "        return logs\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        logs = self.step(batch, batch_idx)\n",
    "        self.log_dict(logs, on_step=False, on_epoch=True)\n",
    "        return logs['loss_train']\n",
    "\n",
    "    # validation logics (is evaluated during the training, but the data is not used to the optimization loop)\n",
    "    def validation_step(self, batch, batch_idx) :\n",
    "        logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({'loss_validation' : logs['loss_train']}, on_step=False, on_epoch=True)\n",
    "    \n",
    "    # gives model output in a form of a dictionary of batches of 2d fields\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx) :\n",
    "        x = transform_features(batch, self.input_features, self.torch_model.cut_border_pix_input)\n",
    "        \n",
    "        output_tensor = self.torch_model(x)\n",
    "        if (self.data_geometry == '2D') :\n",
    "            batch_len, nb_of_channels, output_h, output_w = output_tensor.shape\n",
    "        if (self.data_geometry == '3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = output_tensor.shape\n",
    "\n",
    "        if not(self.output_units is None) : # if output of the model is dimensionless -> compute output with physical units\n",
    "            y_units = transform_features(batch, self.output_units, self.torch_model.cut_border_pix_output)\n",
    "            output_tensor_units = output_tensor*y_units\n",
    "            \n",
    "        # construct the dictionary of the predicted features by decomposing the channels into dictionary entities\n",
    "        pred = dict()\n",
    "        if (self.data_geometry == '2D') :\n",
    "            channel_dim = 1\n",
    "        if (self.data_geometry == '3D') :\n",
    "            channel_dim = 2\n",
    "        for i, feature in enumerate(self.output_features) :\n",
    "            if (self.output_units is None) :\n",
    "                pred[feature] = output_tensor.select(dim=channel_dim, index=i)\n",
    "            else :\n",
    "                pred[feature+'_dimless'] = output_tensor.select(dim=channel_dim, index=i)\n",
    "                pred[feature] = output_tensor_units.select(dim=channel_dim, index=i)\n",
    "            # if some outputs are normalized then compute also result in the restored units (not normalized)\n",
    "            if feature.startswith('normalized_') :\n",
    "                not_normalized_feature_name = feature.replace(\"normalized_\", \"\")\n",
    "                pred[not_normalized_feature_name] = PyLiDataModule.tensor_restore_norm(pred[feature], batch, not_normalized_feature)\n",
    "        \n",
    "        # save the mask and masked outputs (use the eroded mask)\n",
    "        for i, feature in enumerate(self.list_of_features_to_predict) :\n",
    "            if (self.data_geometry == '2D') :\n",
    "                 pred['mask'] = batch['eroded_mask']\n",
    "            if (self.data_geometry == '3D') :\n",
    "                pred['mask'] = batch['eroded_mask'][:,None,:,:]\n",
    "            pred[feature+'_masked'] = expand_to_bords(pred[feature], self.torch_model.cut_border_pix_output)\n",
    "            pred[feature+'_masked'] = pred[feature+'_masked'].where(pred['mask'], torch.ones_like(pred[feature+'_masked'])*np.nan)\n",
    "        return pred \n",
    "    \n",
    "    # testing logic - to evaluate the model after training\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx) :\n",
    "        pred = self.predict_step(batch, batch_idx, dataloader_idx)\n",
    "        mask = cut_bords(batch['eroded_mask'], self.torch_model.cut_border_pix_output)\n",
    "        \n",
    "        test_dict = dict({'loss_val' : dict(), 'loss_grad' : dict(), 'corr_coef' : dict(), 'corr_coef_grad' : dict()})\n",
    "        dict_for_log = dict()\n",
    "        \n",
    "        # global metrics\n",
    "        for i, feature in enumerate(self.list_of_features_to_predict) :\n",
    "            truth = cut_bords(batch[feature], self.torch_model.cut_border_pix_output)\n",
    "            model_output = pred[feature]  # use unmasked prediction here, mask will be applied further on error tensor\n",
    "            \n",
    "            test_dict['loss_val'][feature] = evaluate_tensor_metrics_with_mask(self.data_geometry, torch.nn.functional.mse_loss, \\\n",
    "                                                                                    mask, model_output, truth, reduction='mean')\n",
    "            test_dict['corr_coef'][feature] = torch.corrcoef(torch.vstack((torch.flatten(model_output).view(1,-1), \\\n",
    "                                                              torch.flatten(truth).view(1,-1))))[1,0]\n",
    "            # metrics on horizontal gradients\n",
    "            model_output_grad = finite_diffs_sqr_2d_map(model_output)\n",
    "            truth_grad = finite_diffs_sqr_2d_map(truth)\n",
    "            test_dict['loss_grad'][feature] = evaluate_tensor_metrics_with_mask(self.data_geometry, torch.nn.functional.mse_loss, \\\n",
    "                                                                                     mask[:,1:-1,1:-1], model_output_grad, \\\n",
    "                                                                                     truth_grad, reduction='mean')\n",
    "            test_dict['corr_coef_grad'][feature] = torch.corrcoef(torch.vstack((torch.flatten(model_output_grad).view(1,-1), \\\n",
    "                                                              torch.flatten(truth_grad).view(1,-1))))[1,0]\n",
    "\n",
    "        # pressure at 100th level\n",
    "        if (self.data_geometry == '3D') :\n",
    "            idx_level = 100\n",
    "            true_votemper_var = cut_bords(batch['votemper_var'], self.torch_model.cut_border_pix_output)\n",
    "            rho_ct_ct = cut_bords(batch['rho_ct_ct'], self.torch_model.cut_border_pix_output)\n",
    "            dx = cut_bords(batch['e1t'], self.torch_model.cut_border_pix_output)\n",
    "            dy = cut_bords(batch['e2t'], self.torch_model.cut_border_pix_output)\n",
    "            model_votemper_var = pred['votemper_var']\n",
    "            true_pres_grad_x, true_pres_grad_y, true_pres_grad_norm = get_pressure_grad(true_votemper_var, rho_ct_ct, dx, dy)\n",
    "            pred_pres_grad_x, pred_pres_grad_y, pred_pres_grad_norm = get_pressure_grad(model_votemper_var, rho_ct_ct, dx, dy)\n",
    "            test_dict['loss_val']['pressure_grad_x'] = evaluate_tensor_metrics_with_mask(self.data_geometry, torch.nn.functional.mse_loss, mask[:,1:-1,1:-1], \\\n",
    "                                                                             pred_pres_grad_x[:,idx_level,:,:], true_pres_grad_x[:,idx_level,:,:], reduction='mean')\n",
    "            test_dict['loss_val']['pressure_grad_y'] = evaluate_tensor_metrics_with_mask(self.data_geometry, torch.nn.functional.mse_loss, mask[:,1:-1,1:-1], \\\n",
    "                                                                             pred_pres_grad_y[:,idx_level,:,:], true_pres_grad_y[:,idx_level,:,:], reduction='mean')\n",
    "            test_dict['loss_val']['pressure_grad'] = test_dict['loss_val']['pressure_grad_x'] + test_dict['loss_val']['pressure_grad_y']\n",
    "        \n",
    "        for metrics in list(test_dict.keys()) : \n",
    "            for feature in list(test_dict[metrics].keys()) : \n",
    "                dict_for_log.update({(metrics+'_'+feature) : test_dict[metrics][feature]})\n",
    "        self.log_dict(dict_for_log)\n",
    "\n",
    "    def configure_optimizers(self) :\n",
    "        optimizer = self.optimizer(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685e749-f3b5-4140-9549-ba391d1902f6",
   "metadata": {},
   "source": [
    "## Torch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293381a9-a6f9-40df-a5fd-f31c67548398",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d9428fc0-e221-4fc5-815e-37d30b9ba01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_regr_model(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.nb_of_input_features = nb_of_input_features\n",
    "        self.nb_of_output_features = nb_of_output_features\n",
    "        \n",
    "        self.cut_border_pix_output = None\n",
    "        self.cut_border_pix_input = None\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(self.nb_of_input_features, self.nb_of_output_features, bias=False)\n",
    "        \n",
    "        # initialization \n",
    "        self.lin1.weight.data = torch.Tensor([[0.1]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (self.data_geometry == '3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = x.shape\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "            new_batch_len = batch_len*nb_of_levels\n",
    "        if (self.data_geometry == '2D') :\n",
    "            new_batch_len, nb_of_channels, output_h, output_w = x.shape\n",
    "            res = x \n",
    "        \n",
    "        # first split the input 4D torch tensor into individual pixels (equivalent to patches of size 1x1)\n",
    "        res = torch.nn.functional.unfold(res, kernel_size=1, dilation=1, padding=0, stride=1)\n",
    "        res = torch.permute(res, dims=(0,2,1))\n",
    "        res = torch.flatten(res, end_dim=1).to(torch.float32)\n",
    "        \n",
    "        # perform linear regression\n",
    "        res = self.lin1(res)\n",
    "        \n",
    "        # reshape the model output back to a 4D torch tensor\n",
    "        res = torch.permute(res.unflatten(dim=0, sizes=[new_batch_len,-1]),dims=(0,2,1))\n",
    "        res = torch.nn.functional.fold(res, output_size=(output_h,output_w), kernel_size=1, dilation=1, padding=0, stride=1)\n",
    "        \n",
    "        if (self.data_geometry == '3D') :\n",
    "            # unflatten the levels back\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fe844-bd44-405a-bbf1-b623c9413acf",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e0c83d9-b648-42bd-8a76-c5dcfe0fbf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features, input_patch_size, output_patch_size, int_layer_width=50):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.input_patch_size = input_patch_size\n",
    "        self.output_patch_size = output_patch_size\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(nb_of_input_features*input_patch_size**2, int_layer_width, bias=True)\n",
    "        self.lin2 = torch.nn.Linear(int_layer_width, int_layer_width, bias=True)\n",
    "        self.lin3 = torch.nn.Linear(int_layer_width, nb_of_output_features*output_patch_size**2, bias=True)\n",
    "        \n",
    "        self.cut_border_pix_output = self.input_patch_size//2 - self.output_patch_size//2\n",
    "        if (self.cut_border_pix_output < 1) :\n",
    "            self.cut_border_pix_output = None\n",
    "        self.cut_border_pix_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (self.data_geometry =='3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels = x.shape[0:3]\n",
    "            output_h = x.shape[3]-2*(self.cut_border_pix_output or 0)\n",
    "            output_w = x.shape[4]-2*(self.cut_border_pix_output or 0)\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "            new_batch_len = batch_len*nb_of_levels\n",
    "        if (self.data_geometry =='2D') :\n",
    "            new_batch_len, nb_of_channels = x.shape[0:2]\n",
    "            output_h = x.shape[2]-2*(self.cut_border_pix_output or 0)\n",
    "            output_w = x.shape[3]-2*(self.cut_border_pix_output or 0)\n",
    "            res = x\n",
    "        \n",
    "        # create patches of size 'input_patch_size' and join them into batches (zero padding - will remove border pixels)\n",
    "        res = torch.nn.functional.unfold(res, kernel_size=self.input_patch_size, dilation=1, padding=0, stride=1)\n",
    "        res = torch.permute(res, dims=(0,2,1))\n",
    "        res = torch.flatten(res, end_dim=1)\n",
    "        \n",
    "        # pass though the FCNN\n",
    "        res = self.lin1(res)\n",
    "        res = torch.nn.functional.relu(res)\n",
    "        res = self.lin2(res)\n",
    "        res = torch.nn.functional.relu(res)\n",
    "        res = self.lin3(res)\n",
    "        \n",
    "        # reshape the output patches back into a 4D torch tensor\n",
    "        res = res.unflatten(dim=0, sizes=(new_batch_len,-1))\n",
    "        res = torch.permute(res,dims=(0,2,1))\n",
    "        res = torch.nn.functional.fold(res, output_size=(output_h,output_w), \\\n",
    "                                       kernel_size=self.output_patch_size, dilation=1, padding=0, stride=1)\n",
    "        # compute the divider needed to get correct values in case of overlapping patches (will give mean over all overlapping patches)\n",
    "        mask_ones = torch.ones((1,1,output_h,output_w)).to(x.device)\n",
    "        divisor = torch.nn.functional.fold(torch.nn.functional.unfold(mask_ones, kernel_size=self.output_patch_size), \\\n",
    "                                           kernel_size=self.output_patch_size, output_size=(output_h,output_w))   \n",
    "        res = res/divisor.view(1,1,output_h,output_w)\n",
    "        \n",
    "        if (self.data_geometry =='3D') :\n",
    "            # unflatten the levels\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187ee78-26ad-4d0e-b6d5-305005316106",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74881638-7fc9-4f4b-aec0-32aa60ce0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features, padding='same', padding_mode='replicate', \\\n",
    "                 kernel_size=3, int_layer_width=64):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_mode = 'replicate'\n",
    "        \n",
    "        self.cut_border_pix_input = None\n",
    "        if self.padding == 'same' :\n",
    "            self.cut_border_pix_output = self.cut_border_pix_input\n",
    "        if self.padding == 'valid' :\n",
    "            self.cut_border_pix_output = (self.cut_border_pix_input or 0) + self.kernel_size//2\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=nb_of_input_features, out_channels=int_layer_width, kernel_size=self.kernel_size, \\\n",
    "                                     padding=self.padding,  padding_mode=self.padding_mode) \n",
    "        self.conv2 = torch.nn.Conv2d(int_layer_width, int_layer_width, kernel_size=self.kernel_size, padding='same', padding_mode=self.padding_mode) \n",
    "        self.conv3 = torch.nn.Conv2d(int_layer_width, nb_of_output_features, kernel_size=self.kernel_size, padding='same', \\\n",
    "                                     padding_mode=self.padding_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_len = x.shape[0]\n",
    "        if (self.data_geometry == '3D') :\n",
    "            nb_of_levels = x.shape[1]\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "        else :\n",
    "            res = x\n",
    "        \n",
    "        res = self.conv1(res)\n",
    "        res = torch.nn.functional.relu(res)\n",
    "        res = self.conv2(res)\n",
    "        res = torch.nn.functional.relu(res)\n",
    "        res = self.conv3(res)\n",
    "        \n",
    "        if (self.data_geometry == '3D') :\n",
    "            # unflatten the levels\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        return res       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d97b74-61d1-43d2-9d9e-3b825250c8b6",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c71bf7a-9728-4235-a85e-a6c03599113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, project) :\n",
    "    wandb_logger = WandbLogger(name=config['model_label']+'_'+config['version'], \\\n",
    "                               version=config['model_label']+'_'+config['version'],\\\n",
    "                               project=project, config=config, resume=False, log_model=False, offline=True)\n",
    "    \n",
    "    torch_model = eval(config['torch_model'])(**config['torch_model_params'])\n",
    "    pylight_module = GenericPyLiModule(torch_model, **config['module_params'])\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"loss_train\", save_last=True)    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"loss_train\", mode=\"min\")\n",
    "    \n",
    "    trainer = pl.Trainer(**config['training_params'], logger=wandb_logger, \n",
    "                     callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                     accelerator='gpu', devices=(1 if torch.cuda.is_available() else None))  \n",
    "    trainer.fit(model = pylight_module, datamodule=eval(config['datamodule']))\n",
    "    #tests\n",
    "    test_datamodule = eval(config['datamodule'])\n",
    "    test_datamodule.setup(stage='test')\n",
    "    trainer.test(model = pylight_module, datamodule=test_datamodule)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de8768-4f7f-4c10-af64-d618b8b4230e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88828ed-5e5b-452b-aab8-da0947b420d1",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76db4448-d28a-4270-bb64-6de61cd4e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_BUCKET = os.environ['PERSISTENT_BUCKET'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30f449a2-50f9-494d-bf4c-77f853392e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict=[dict() for i in range(2)]\n",
    "data_dict[0] = dict({'region' : '1', 'season' : 'fma', 'label' : 'GULFSTR FMA'})\n",
    "data_dict[1] = dict({'region' : '1', 'season' : 'aso', 'label' : 'GULFSTR ASO'})\n",
    "# data_dict[2] = dict({'region' : '2', 'season' : 'fma', 'label' : 'MIDATL FMA'})\n",
    "# data_dict[3] = dict({'region' : '2', 'season' : 'aso', 'label' : 'MIDATL ASO'})\n",
    "# data_dict[4] = dict({'region' : '3', 'season' : 'fma', 'label' : 'WESTMED FMA'})\n",
    "# data_dict[5] = dict({'region' : '3', 'season' : 'aso', 'label' : 'WESTMED ASO'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa0e4b6-6c0d-4cce-95c0-223506fa3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "height = 45\n",
    "width = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34bb9812-17c9-41ee-83d6-46221e925bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.51 s, sys: 1.62 s, total: 9.13 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_to_add_to_sample = ['votemper', 'votemper_var', 'rho_ct_ct', 'diff_votemper_sqr']\n",
    "auxiliary_features = ['z_l', 'f', 'e1t', 'e2t']\n",
    "all_data_3D = PyLiDataModule(data_dict, '3D', features_to_add_to_sample, auxiliary_features, height, width, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b42b115b-6c02-4dd4-8e95-4059c37b917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 707 ms, sys: 146 ms, total: 853 ms\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_to_add_to_sample = ['sosstsst', 'sosstsst_var', 'rho_ct_ct', 'diff_sosstsst_sqr']\n",
    "auxiliary_features = ['e1t', 'e2t']\n",
    "all_data_2D = PyLiDataModule(data_dict, '2D', features_to_add_to_sample, auxiliary_features, height, width, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "04341a50-38c4-4c47-ab3b-8636903857a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_2D.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8ca341b0-0750-427b-9617-4d6ada9fb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_2D.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bc491c31-1b9b-4c61-84c9-0f2c2e35d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = all_data_2D.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90a2a134-3304-4edb-8713-d9cf64de1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_di = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "93260680-4519-472b-bdd5-2b0086d0742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(test_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7e084999-a2e5-464e-b3bd-919694c662af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 43, 38])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "central_diffs(sample['sosstsst'])[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb222872-8f0c-49b1-a4bb-609b91832511",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dcb94fb-1f4c-44fc-a746-fd938e31bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "62aaf6ff-5b20-46a9-833c-a3d7326f87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3cebc87e-7278-4bdf-a1f1-a65f2b48d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/CNN_CNN_kernel3_3D/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type | Params\n",
      "-------------------------------------\n",
      "0 | torch_model | FCNN | 4.3 K \n",
      "-------------------------------------\n",
      "4.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8888d029834001bd389577e66454db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0807cc689eb2452895a311df508f3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">         Test metric         </span>┃<span style=\"font-weight: bold\">        DataLoader 0         </span>┃<span style=\"font-weight: bold\">        DataLoader 1         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> corr_coef_grad_votemper_var </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.6258432865142822      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.436993807554245      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   corr_coef_votemper_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.623487114906311      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.5482894778251648      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   loss_grad_votemper_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.9690124988555908      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.6708462238311768      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   loss_val_pressure_grad    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   1.0135454035824308e-07    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   1.0877562549277112e-07    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  loss_val_pressure_grad_x   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    7.997983972434121e-08    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    9.10664269867404e-08     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  loss_val_pressure_grad_y   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.137470063390187e-08    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.770919850603072e-08    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_val_votemper_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.09202902019023895     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.05515830218791962     </span>│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m        Test metric        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 0        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 1        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mcorr_coef_grad_votemper_var\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.6258432865142822     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.436993807554245     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  corr_coef_votemper_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.623487114906311     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.5482894778251648     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  loss_grad_votemper_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.9690124988555908     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.6708462238311768     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  loss_val_pressure_grad   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  1.0135454035824308e-07   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  1.0877562549277112e-07   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m loss_val_pressure_grad_x  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   7.997983972434121e-08   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   9.10664269867404e-08    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m loss_val_pressure_grad_y  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.137470063390187e-08   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.770919850603072e-08   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_val_votemper_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.09202902019023895    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.05515830218791962    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_votemper_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_votemper_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_votemper_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_votemper_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>first_weight</td><td>█▁</td></tr><tr><td>loss_grad_votemper_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_votemper_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_pressure_grad_x/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_pressure_grad_x/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_pressure_grad_y/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_pressure_grad_y/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_votemper_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_votemper_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_validation</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_votemper_var/dataloader_idx_0</td><td>0.62584</td></tr><tr><td>corr_coef_grad_votemper_var/dataloader_idx_1</td><td>0.43699</td></tr><tr><td>corr_coef_votemper_var/dataloader_idx_0</td><td>0.62349</td></tr><tr><td>corr_coef_votemper_var/dataloader_idx_1</td><td>0.54829</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>first_weight</td><td>0.03793</td></tr><tr><td>loss_grad_votemper_var/dataloader_idx_0</td><td>0.96901</td></tr><tr><td>loss_grad_votemper_var/dataloader_idx_1</td><td>0.67085</td></tr><tr><td>loss_train</td><td>0.1298</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_0</td><td>0.0</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_1</td><td>0.0</td></tr><tr><td>loss_val_pressure_grad_x/dataloader_idx_0</td><td>0.0</td></tr><tr><td>loss_val_pressure_grad_x/dataloader_idx_1</td><td>0.0</td></tr><tr><td>loss_val_pressure_grad_y/dataloader_idx_0</td><td>0.0</td></tr><tr><td>loss_val_pressure_grad_y/dataloader_idx_1</td><td>0.0</td></tr><tr><td>loss_val_votemper_var/dataloader_idx_0</td><td>0.09203</td></tr><tr><td>loss_val_votemper_var/dataloader_idx_1</td><td>0.05516</td></tr><tr><td>loss_validation</td><td>0.07421</td></tr><tr><td>trainer/global_step</td><td>52</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230314_190204-CNN_CNN_kernel3_3D<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230314_190204-CNN_CNN_kernel3_3D/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config7=dict({'model_label' : 'FCNN',\n",
    "                'version' : 'FCNN_kernel3_3D',\n",
    "                'torch_model' : 'FCNN',\n",
    "                'datamodule' : 'all_data_3D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '3D',\\\n",
    "                                            'nb_of_input_features' : 1, \\\n",
    "                                            'nb_of_output_features' : 1, \\\n",
    "                                            'input_patch_size' : 5,\n",
    "                                            'output_patch_size' : 3, \n",
    "                                            'int_layer_width' : 50}),\n",
    "                'module_params' : dict({'input_features'  : ['normalized_votemper'],\n",
    "                                        'output_features'  : ['votemper_var'],\n",
    "                                        'output_units' : ['filtered_diff_votemper_sqr'],\n",
    "                                        'loss' : torch.nn.functional.mse_loss,\n",
    "                                        'optimizer' : torch.optim.Adam,\n",
    "                                        'learning_rate' : 1e-3,}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config7, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e77c0afd-9160-45d4-9917-648ee21e7c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/FCNN_FCNN_2D/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type | Params\n",
      "-------------------------------------\n",
      "0 | torch_model | FCNN | 4.3 K \n",
      "-------------------------------------\n",
      "4.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e8319fd7cf48a387dfb59dccb95ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492a24ec9c324774a1c6a6ca185a0101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">         Test metric         </span>┃<span style=\"font-weight: bold\">        DataLoader 0         </span>┃<span style=\"font-weight: bold\">        DataLoader 1         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> corr_coef_grad_sosstsst_var </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8575488924980164      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8347499370574951      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   corr_coef_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8379396796226501      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8769893646240234      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   loss_grad_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      11.6495361328125       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.5395290851593018      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_val_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.25913771986961365     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.05558266490697861     </span>│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m        Test metric        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 0        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 1        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mcorr_coef_grad_sosstsst_var\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8575488924980164     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8347499370574951     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  corr_coef_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8379396796226501     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8769893646240234     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  loss_grad_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     11.6495361328125      \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.5395290851593018     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_val_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.25913771986961365    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.05558266490697861    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>first_weight</td><td>▁█</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_validation</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_0</td><td>0.85755</td></tr><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_1</td><td>0.83475</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_0</td><td>0.83794</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_1</td><td>0.87699</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>first_weight</td><td>-0.17818</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_0</td><td>11.64954</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_1</td><td>0.53953</td></tr><tr><td>loss_train</td><td>0.15932</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_0</td><td>0.25914</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_1</td><td>0.05558</td></tr><tr><td>loss_validation</td><td>0.15573</td></tr><tr><td>trainer/global_step</td><td>1302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230314_190426-FCNN_FCNN_2D<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230314_190426-FCNN_FCNN_2D/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config7=dict({'model_label' : 'FCNN',\n",
    "                'version' : 'FCNN_2D',\n",
    "                'torch_model' : 'FCNN',\n",
    "                'datamodule' : 'all_data_2D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '2D',\\\n",
    "                                            'nb_of_input_features' : 1, \\\n",
    "                                            'nb_of_output_features' : 1, \\\n",
    "                                            'input_patch_size' : 5,\n",
    "                                            'output_patch_size' : 3, \n",
    "                                            'int_layer_width' : 50}),\n",
    "                'module_params' : dict({'input_features'  : ['normalized_sosstsst'],\n",
    "                                        'output_features'  : ['sosstsst_var'],\n",
    "                                        'output_units' : ['filtered_diff_sosstsst_sqr'],\n",
    "                                        'loss' : torch.nn.functional.mse_loss,\n",
    "                                        'optimizer' : torch.optim.Adam,\n",
    "                                        'learning_rate' : 1e-3,}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config7, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ce4f330f-1733-445f-a660-d5a4d2388683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/CNN_CNN_2D/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type | Params\n",
      "-------------------------------------\n",
      "0 | torch_model | CNN  | 23.5 K\n",
      "-------------------------------------\n",
      "23.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 K    Total params\n",
      "0.094     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecb31e342d146f1a2812c04eb0d9153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0183b38cb1d14cb7bc89d40909a763fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">         Test metric         </span>┃<span style=\"font-weight: bold\">        DataLoader 0         </span>┃<span style=\"font-weight: bold\">        DataLoader 1         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> corr_coef_grad_sosstsst_var </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.837498664855957      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8386675119400024      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   corr_coef_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8546265363693237      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.8848034739494324      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   loss_grad_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     10.477635383605957      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.5569102764129639      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_val_sosstsst_var    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.24202285706996918     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.05318509787321091     </span>│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m        Test metric        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 0        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 1        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mcorr_coef_grad_sosstsst_var\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.837498664855957     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8386675119400024     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  corr_coef_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8546265363693237     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.8848034739494324     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  loss_grad_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    10.477635383605957     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.5569102764129639     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_val_sosstsst_var   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.24202285706996918    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.05318509787321091    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────┴─────────────────────────────┴─────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>first_weight</td><td>▁█</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_validation</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_0</td><td>0.8375</td></tr><tr><td>corr_coef_grad_sosstsst_var/dataloader_idx_1</td><td>0.83867</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_0</td><td>0.85463</td></tr><tr><td>corr_coef_sosstsst_var/dataloader_idx_1</td><td>0.8848</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>first_weight</td><td>0.23789</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_0</td><td>10.47764</td></tr><tr><td>loss_grad_sosstsst_var/dataloader_idx_1</td><td>0.55691</td></tr><tr><td>loss_train</td><td>0.16405</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_0</td><td>0.24202</td></tr><tr><td>loss_val_sosstsst_var/dataloader_idx_1</td><td>0.05319</td></tr><tr><td>loss_validation</td><td>0.1461</td></tr><tr><td>trainer/global_step</td><td>1302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230314_190515-CNN_CNN_2D<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230314_190515-CNN_CNN_2D/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config7=dict({'model_label' : 'CNN',\n",
    "                'version' : 'CNN_2D',\n",
    "                'torch_model' : 'CNN',\n",
    "                'datamodule' : 'all_data_2D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '2D',\\\n",
    "                                            'nb_of_input_features' : 1, \\\n",
    "                                            'nb_of_output_features' : 1, \\\n",
    "                                            'int_layer_width' : 50}),\n",
    "                'module_params' : dict({'input_features'  : ['normalized_sosstsst'],\n",
    "                                        'output_features'  : ['sosstsst_var'],\n",
    "                                        'output_units' : ['filtered_diff_sosstsst_sqr'],\n",
    "                                        'loss' : torch.nn.functional.mse_loss,\n",
    "                                        'optimizer' : torch.optim.Adam,\n",
    "                                        'learning_rate' : 1e-3,}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config7, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755998d8-0013-4fc7-90fa-6b0686535602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
