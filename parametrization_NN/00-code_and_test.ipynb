{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb7152a-adfe-4999-8ca0-77e6536009a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7529750b-aea0-4a88-98f6-641587ce401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:46547\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36532fb-c2ed-43b3-8bd0-7c1086407d8a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07708f7a-4d51-459b-ac79-fe856c93a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from scipy import ndimage\n",
    "import itertools\n",
    "import os\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,Callback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9717e6-2e96-4ad4-bc6e-2490ad6ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux-5.10.133+-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf23faa3-d7e0-4a71-bf09-67a11edff645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1.post200\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d23fce9-db7f-4af5-926b-085a045bc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c3028-781a-46cd-a308-635207e8f62e",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931cafd-2d13-413d-9048-6db903c72e38",
   "metadata": {},
   "source": [
    "## Useful xArray functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2239000-2adb-4edb-952e-517537d33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_2d_maps(xr_data, h, w) :\n",
    "    return xr_data.isel(x_c=slice(None,w), y_c=slice(None,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f889bc9-1159-4e74-8219-5b8f99aca7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_bin_mask(xr_mask) :\n",
    "    erosion_structure_matrix = np.array([(0,0,1,0,0), (0,1,1,1,0), (1,1,1,1,1), (0,1,1,1,0), (0,0,1,0,0)])\n",
    "    np_array_mask = ndimage.binary_erosion(xr_mask, structure=erosion_structure_matrix)\n",
    "    return xr_mask.copy(data=np_array_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847fdf9-c703-42bd-a692-3670ca67fd11",
   "metadata": {},
   "source": [
    "## Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e087d1c-e06b-44e8-a01b-520a4b204f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset of 2D maps of surface temperature, salinity\"\"\"\n",
    "\n",
    "    def __init__(self, xarray_dataset, features_to_add_to_sample, auxiliary_features, height, width):\n",
    "        self.features_to_add_to_sample = features_to_add_to_sample\n",
    "        self.auxiliary_features = auxiliary_features\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.data = crop_2d_maps(xarray_dataset, self.height, self.width).load()\n",
    "        self.data_file_len = len(self.data.t)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_file_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            list_idx = idx.tolist()\n",
    "        else :\n",
    "            list_idx = idx\n",
    "        selected_time_frames = self.data.isel(t=list_idx) #still xArray object\n",
    "        \n",
    "        # create dictionary of a sample (a batch) containig different features in numpy format. \n",
    "        # This dictionary is an intermediate step, preparing xArray data for transform into pytorch tensors\n",
    "        sample = dict()\n",
    "        sample['mask'] = toTorchTensor(selected_time_frames['mask'].astype(bool))\n",
    "        sample['eroded_mask'] = toTorchTensor(erode_bin_mask(selected_time_frames['mask']))\n",
    "        \n",
    "        for feature in self.features_to_add_to_sample :\n",
    "            sample['mean_'+feature] = toTorchTensor(self.data['mean_'+feature])\n",
    "            sample['std_'+feature] = toTorchTensor(self.data['std_'+feature])\n",
    "            sample[feature] = toTorchTensor(selected_time_frames[feature])\n",
    "\n",
    "        for feature in self.auxiliary_features :\n",
    "            sample[feature] = toTorchTensor(selected_time_frames[feature])\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98fc0589-8400-4760-a827-0d8f822359c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTorchTensor(xrArray):\n",
    "    transformed_data = torch.tensor(xrArray.values)\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1323fc6-fc0c-4cc5-942a-ea87e895525f",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af829e9f-7769-4b2b-aa56-ad2d13dfc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyLiDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, cloud_data_sets, data_geometry, features_to_add_to_sample, auxiliary_features, height, width, batch_size) :\n",
    "        super().__init__()\n",
    "        self.cloud_data_sets = cloud_data_sets\n",
    "        self.data_geometry = data_geometry\n",
    "        self.features_to_add_to_sample = features_to_add_to_sample\n",
    "        self.auxiliary_features = auxiliary_features\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.batch_size = batch_size\n",
    "        self.list_of_xr_datasets = [xr.Dataset() for i in range(len(self.cloud_data_sets))]\n",
    "        self.list_of_torch_datasets = [{} for i in range(len(self.cloud_data_sets))]\n",
    "        \n",
    "    #def prepare_data(self) :\n",
    "        # preparation of data: mean and std of the dataset (to avoid batch avg), normalization and nan filling\n",
    "        for i in range(len(self.cloud_data_sets)) :\n",
    "            # read file\n",
    "            PERSISTENT_BUCKET = os.environ['PERSISTENT_BUCKET'] \n",
    "            if (self.data_geometry =='2D') :\n",
    "                file_prefix = 'data'\n",
    "            if (self.data_geometry =='3D') :\n",
    "                file_prefix = 'data3D_'\n",
    "            xr_dataset = xr.open_zarr(f'{PERSISTENT_BUCKET}/'+file_prefix+str(i)+'.zarr', chunks='auto')\n",
    "            rename_rules_dictionary = dict({'votemper':'temp', 'sosstsst':'temp', 'vosaline' : 'saline', 'sosaline' : 'saline'})\n",
    "            for name_to_replace, new_name in rename_rules_dictionary.items() :\n",
    "                for var in xr_dataset.variables :\n",
    "                    if (name_to_replace in var):\n",
    "                        new_var_name = var.replace(name_to_replace, new_name)\n",
    "                        xr_dataset = xr_dataset.rename({var : new_var_name})\n",
    "            xr_dataset = xr_dataset[self.features_to_add_to_sample + self.auxiliary_features + ['mask']]\n",
    "            for feature in self.features_to_add_to_sample :\n",
    "                # reapply mask (to avoid issues with nans written in netcdf files)\n",
    "                xr_dataset[feature] = xr_dataset[feature].where(xr_dataset.mask>0)\n",
    "                # compute mean, median and std for each level (since temperature/salinity may change a lot with the depth)\n",
    "                xr_dataset['mean_'+feature] = (xr_dataset[feature].mean(dim=['t', 'x_c', 'y_c']))\n",
    "                xr_dataset['std_'+feature] = (xr_dataset[feature].std(dim=['t', 'x_c', 'y_c']))\n",
    "                # fill nans with mean (doesn't the number to be fillted in matter since they will be masked, \n",
    "                # but they have to be filled with any numbers so that nans do not propagate everywhere) \n",
    "                xr_dataset[feature] = xr_dataset[feature].fillna(xr_dataset['mean_'+feature])\n",
    "            # save result in a list\n",
    "            self.list_of_xr_datasets[i] = xr_dataset\n",
    "            self.list_of_torch_datasets[i] = torchDataset(xr_dataset, self.features_to_add_to_sample, self.auxiliary_features, self.height, self.width)\n",
    "            \n",
    "    def setup(self, stage: str) :\n",
    "        if (stage == 'fit') :\n",
    "        # takes first 60% of time snapshots for training\n",
    "            self.train_dataset = torch.utils.data.ConcatDataset([torch.utils.data.Subset(dataset, \\\n",
    "                                                                                     indices=range(0,int(0.6*len(dataset)))) \\\n",
    "                                                                                     for dataset in self.list_of_torch_datasets])\n",
    "        # takes last 20% of time snapshots for validation (we keep a gap to have validation data decorrelated from trainig data)\n",
    "            self.val_dataset = torch.utils.data.ConcatDataset([torch.utils.data.Subset(dataset, \\\n",
    "                                                                                     indices=range(int(0.8*len(dataset)),len(dataset))) \\\n",
    "                                                                                     for dataset in self.list_of_torch_datasets])\n",
    "        # same for test\n",
    "        if (stage == 'test') :\n",
    "            self.test_datasets_byregion = [torch.utils.data.Subset(dataset, indices=range(int(0.8*len(dataset)),len(dataset))) \\\n",
    "                                                               for dataset in self.list_of_torch_datasets]\n",
    "            self.test_dataset_all_data = torch.utils.data.ConcatDataset([torch.utils.data.Subset(dataset, \\\n",
    "                                                                                     indices=range(int(0.8*len(dataset)),len(dataset))) \\\n",
    "                                                                                     for dataset in self.list_of_torch_datasets])\n",
    "            \n",
    "    def train_dataloader(self) :\n",
    "        # create training dataloadder from train_dataset with shuffling with given batch size\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, \\\n",
    "                                           batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "    \n",
    "    def val_dataloader(self) :\n",
    "        # create training dataloadder from val_dataset without shuffling with the same batch size\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, drop_last=True, num_workers=0) \n",
    "    \n",
    "    def test_dataloader(self) :\n",
    "        # create a LIST of dataloaders (a dataloader for each dataset) - to enable diagnostics in each region/season individually \n",
    "        # batch size is equal to the dataset length, i.e. there is ONLY 1 batch with all dataset inside (can be better since there is no optimisation in testing)\n",
    "        return ([torch.utils.data.DataLoader(dataset, batch_size=len(dataset), drop_last=True, num_workers=0) for dataset in self.test_datasets_byregion] \n",
    "                + [torch.utils.data.DataLoader(self.test_dataset_all_data, batch_size=len(self.test_dataset_all_data), num_workers=0)])\n",
    "    \n",
    "    def teardown(self, stage : str) :\n",
    "        if (stage == 'fit') :\n",
    "            # clean train and val datasets to free memory\n",
    "            del self.train_dataset, self.val_dataset\n",
    "        # if (stage == 'test') :\n",
    "        #     del self.test_datasets   \n",
    "        # if (stage == 'predict') :\n",
    "        #     del self.test_datasets   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f231e-97bb-4509-8cf1-0858d533a8b4",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ad404-3303-48e0-8c58-faae3f1a7f62",
   "metadata": {},
   "source": [
    "### Differences and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04af39e-6d7d-4e0f-9ef8-caa0c398358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_diffs(dataArray) :\n",
    "    if len(dataArray.shape) == 5 : #5d data cube\n",
    "        batch_len, nb_of_levels, nb_of_channels, output_h, output_w = dataArray.shape\n",
    "        flatten_data = dataArray.flatten(start_dim=0, end_dim=2)[:,None,:,:]\n",
    "    if len(dataArray.shape) == 4 : # 1 channel (or 1 level)\n",
    "        batch_len, nb_of_channels, width, height = dataArray.shape\n",
    "        flatten_data = dataArray.flatten(start_dim=0, end_dim=1)[:,None,:,:]\n",
    "    if len(dataArray.shape) == 3 : # 1 channel\n",
    "        batch_len, width, height = dataArray.shape\n",
    "        flatten_data = dataArray[:,None,:,:]\n",
    "    weights = torch.zeros(2,1,3,3).to(dataArray.device) # 2 channels : 1 channel for x-difference, other for y-differences\n",
    "    weights[0,0,:,:] = torch.tensor([[0,0.,0],[-0.5,0.,0.5],[0,0.,0]]) #dx\n",
    "    weights[1,0,:,:] = torch.tensor([[0,-0.5,0],[0,0.,0],[0,0.5,0]])   #dy\n",
    "    res = torch.nn.functional.conv2d(flatten_data.float(), weights, \\\n",
    "                               bias=None, stride=1, padding='same', dilation=1, groups=1)\n",
    "    if len(dataArray.shape) == 5 :\n",
    "        res_dx = res[:,0,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_levels, nb_of_channels))\n",
    "        res_dy = res[:,1,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_levels, nb_of_channels))\n",
    "    if len(dataArray.shape) == 4 :\n",
    "        res_dx = res[:,0,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_channels))\n",
    "        res_dy = res[:,1,1:-1,1:-1].unflatten(dim=0, sizes=(batch_len, nb_of_channels))\n",
    "    if len(dataArray.shape) == 3 :\n",
    "        res_dx = res[:,0,1:-1,1:-1]\n",
    "        res_dy = res[:,1,1:-1,1:-1]\n",
    "    return res_dx, res_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839f508e-157f-4565-a211-66930a230b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diffs_sqr_2d_map(dataArray) :\n",
    "    res_dx, res_dy = central_diffs(dataArray)\n",
    "    res = torch.pow(res_dx,2) + torch.pow(res_dy,2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db91b7-2bc1-43a2-9d32-f77d0a0a3472",
   "metadata": {},
   "source": [
    "### Masked mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da54850c-db47-437e-bbe9-1304cb8ad333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(data_geometry, tensor, mask, reduction='mean') :\n",
    "    if (data_geometry == '2D') :\n",
    "        if (len(tensor.shape) == 4) : # 4D tensor with C features\n",
    "            batch_len, nb_of_channels, output_h, output_w = tensor.shape  \n",
    "            channel_dim = 1\n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_channels\n",
    "            mask = mask[:,None,:,:]\n",
    "        if (len(tensor.shape) == 3) : # 1 feature (=1 channel)-> 3D tensor\n",
    "            batch_len, output_h, output_w = tensor.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)\n",
    "            mask = mask\n",
    "    if (data_geometry == '3D') :\n",
    "        if (len(tensor.shape) == 5) : # full 5D tensor\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = tensor.shape  \n",
    "            channel_dim = 2\n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_levels*nb_of_channels\n",
    "            mask = mask[:,None,None,:,:]\n",
    "        if (len(tensor.shape) == 4) : # 1 feature (=1 channel)\n",
    "            batch_len, nb_of_levels, output_h, output_w = tensor.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)*nb_of_levels\n",
    "            mask = mask[:,None,:,:]\n",
    "        if (len(tensor.shape) == 3) : # 1 feature (=1 channel) and 1 level\n",
    "            batch_len, output_h, output_w = tensor.shape  \n",
    "            valid_mask_counts = torch.count_nonzero(mask)\n",
    "            mask = mask\n",
    "\n",
    "    if (reduction=='none') :\n",
    "        return (tensor*mask)\n",
    "    \n",
    "    total = torch.sum(tensor*mask)\n",
    "    if (reduction=='sum') : \n",
    "        return total    # 1 number\n",
    "    if (reduction=='mean') : \n",
    "        return (total/valid_mask_counts) # 1 number\n",
    "    if (reduction=='vertical_mean') :\n",
    "        sum_over_each_layer = torch.sum(tensor*mask, dim=(-2,-1))\n",
    "        valid_counts_each_layer = torch.count_nonzero(mask, dim=(-2,-1))\n",
    "        vertical_profile_of_each_sample = sum_over_each_layer/valid_counts_each_layer # shape [N,L,(C)]\n",
    "        res = torch.mean(vertical_profile_of_each_sample, dim=0) # average over the batch, final shape [L, (C)]\n",
    "        return res\n",
    "    if (reduction=='horizontal_mean') :\n",
    "        sum_over_depth_at_each_point = torch.sum(tensor*mask, dim=1) # avg over depth, shape [N, (C), H, W]\n",
    "        valid_counts = torch.count_nonzero(mask, dim=1)*nb_of_levels\n",
    "        horizontal_error_of_each_sample = sum_over_depth_at_each_point/valid_counts\n",
    "        res = torch.mean(horizontal_error_of_each_sample, dim=0) # [(C), H, W]\n",
    "        return res \n",
    "    if (reduction=='sample_mean') :\n",
    "        reduc_dims = tuple(range(1,len(tensor.shape))) # all dimensions>0\n",
    "        if not (channel_dim is None) :\n",
    "            reduc_dims.remove(channel_dim) \n",
    "        sum_over_each_sample = torch.sum(tensor*mask, dim=reduc_dims)\n",
    "        valid_counts = torch.count_nonzero(mask, dim=reduc_dims)*(nb_of_levels if data_geometry == '3D' else 1.)\n",
    "        res = sum_over_each_sample/valid_counts # shape[N, (C)]\n",
    "        return res\n",
    "    if (reduction=='channel_mean') : \n",
    "        reduc_dims = list(range(len(tensor.shape)))\n",
    "        reduc_dims.remove(channel_dim)\n",
    "        sum_over_each_channel = torch.sum(tensor*mask, dim=reduc_dims)  \n",
    "        valid_counts = torch.count_nonzero(mask, dim=reduc_dims)*(nb_of_levels if data_geometry == '3D' else 1.)\n",
    "        res = sum_over_each_channel/valid_counts # shape [C]\n",
    "        return res\n",
    "    if (reduction=='normalization_mean') : # by channel, individual for sample (and level if 3D)\n",
    "        sum_over_each_sample = torch.sum(tensor*mask, dim=(-2,-1)) #shape [N, (L), (C)]\n",
    "        valid_counts = torch.count_nonzero(mask, dim=(-2,-1))\n",
    "        mean_of_sample = sum_over_each_sample/valid_counts\n",
    "        res = torch.unsqueeze(torch.unsqueeze(mean_of_sample, dim=-1), dim=-1) # shape [N, (L), (C), 1, 1]\n",
    "        return res "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d8d55-cf8b-4d24-970a-5e9682c42dc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33356835-e629-4d64-ae65-f426b40e89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pressure_grad(temp_var, rho_ct_ct, dx, dy, z_l) :\n",
    "    g = 9.81\n",
    "    dz = torch.diff(z_l)\n",
    "    delta_rho = 0.5*temp_var*rho_ct_ct\n",
    "    dx_rho, dy_rho = central_diffs(delta_rho)\n",
    "    dx_rho = dx_rho[:,:-1,:,:]/dx[:,None,1:-1,1:-1]\n",
    "    dy_rho = dy_rho[:,:-1,:,:]/dy[:,None,1:-1,1:-1]\n",
    "    dx_p = torch.cumsum(dx_rho*g*dz[:,:,None,None], axis=1)   \n",
    "    dy_p = torch.cumsum(dy_rho*g*dz[:,:,None,None], axis=1)\n",
    "    return [dx_p, dy_p, torch.sqrt(torch.pow(dx_p,2)+torch.pow(dy_p,2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d0da6-df75-45ce-8a7f-1ab16ef602d2",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664101b0-a44b-4b06-97c1-eeac3ca48094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_with_mask(data_geometry, metrics, mask, truth, model_output, reduction='mean', normalization=True) :\n",
    "    if normalization :\n",
    "        normalization_coef = masked_mean(data_geometry, truth, mask, reduction='normalization_mean')\n",
    "    else :\n",
    "        normalization_coef = 1.\n",
    "    non_reduced_non_masked_metrics = metrics(model_output/normalization_coef, truth/normalization_coef, reduction='none')\n",
    "    reduced_metrics = masked_mean(data_geometry, non_reduced_non_masked_metrics, mask, reduction=reduction)\n",
    "    return reduced_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8babb33-c059-4eae-960d-4fb751827225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_based_MSEloss(batch, pred_sigma, target_sigma, cut_border_pix, idx_level=100, normalization=True) :\n",
    "    rho_ct_ct = cut_bords(batch['rho_ct_ct'], cut_border_pix)\n",
    "    dx = cut_bords(batch['e1t'], cut_border_pix)\n",
    "    dy = cut_bords(batch['e2t'], cut_border_pix)\n",
    "    z_l = batch['z_l']\n",
    "    mask = cut_bords(batch['eroded_mask'], cut_border_pix)\n",
    "\n",
    "    narrowed_mask = mask[:,1:-1,1:-1] # use narrowed mask since borders are cropped when computing gradient\n",
    "    \n",
    "    true_pres_grad_x, true_pres_grad_y, true_pres_grad_norm = get_pressure_grad(target_sigma, rho_ct_ct, dx, dy, z_l)\n",
    "    pred_pres_grad_x, pred_pres_grad_y, pred_pres_grad_norm = get_pressure_grad(pred_sigma, rho_ct_ct, dx, dy, z_l)\n",
    "\n",
    "    pres_grad_x_loss = evaluate_loss_with_mask('3D', torch.nn.functional.mse_loss, narrowed_mask, \\\n",
    "                                               pred_pres_grad_x[:,idx_level,:,:], true_pres_grad_x[:,idx_level,:,:], \\\n",
    "                                               reduction='mean', normalization=normalization)\n",
    "    pres_grad_y_loss = evaluate_loss_with_mask('3D', torch.nn.functional.mse_loss, narrowed_mask, \\\n",
    "                                               pred_pres_grad_y[:,idx_level,:,:], true_pres_grad_y[:,idx_level,:,:], \\\n",
    "                                               reduction='mean', normalization=normalization)\n",
    "    loss_pres_grad = pres_grad_x_loss+pres_grad_y_loss\n",
    "    return loss_pres_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d661d-3e8f-4f17-822b-cf71602c0055",
   "metadata": {},
   "source": [
    "### Tensor transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c3913f-cff1-4375-b586-bff5ef68a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_bords(tensor, nb_of_border_pix) :\n",
    "    if nb_of_border_pix is None :\n",
    "        return tensor\n",
    "    else :\n",
    "        if (len(tensor.shape) == 5) :\n",
    "            return tensor[:,:, :, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] \n",
    "        if (len(tensor.shape) == 4) :\n",
    "            return tensor[:,:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] \n",
    "        if (len(tensor.shape) == 3) :\n",
    "            return tensor[:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d2e8e7-78ce-443d-bca4-8fb182df26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_bords(tensor, nb_of_border_pix) :\n",
    "    if nb_of_border_pix is None :\n",
    "        return tensor\n",
    "    else :\n",
    "        if (len(tensor.shape) == 4) :\n",
    "            new_tensor = torch.empty((tensor.shape[0],tensor.shape[1], tensor.shape[2]+2*nb_of_border_pix, tensor.shape[3]+2*nb_of_border_pix)).\\\n",
    "            to(tensor.device)\n",
    "            new_tensor[:,:, nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] = tensor\n",
    "        if (len(tensor.shape) == 3) :\n",
    "            new_tensor = torch.empty((tensor.shape[0], tensor.shape[1]+2*nb_of_border_pix, tensor.shape[2]+2*nb_of_border_pix)).to(tensor.device)\n",
    "            new_tensor[:,nb_of_border_pix:-nb_of_border_pix, nb_of_border_pix:-nb_of_border_pix] = tensor        \n",
    "        return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16b6494a-3c08-4b94-afa3-b6e6b495040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_stack_features(batch, features, nb_of_border_pix, normalization_features=None) :\n",
    "    # check if normalization is needed\n",
    "    for index, feature in enumerate(features) :\n",
    "        if feature.startswith('normalized_') :\n",
    "            not_normalized_feature_name = feature.replace(\"normalized_\", \"\")\n",
    "            if (normalization_features is None) :\n",
    "                norm_feature = None\n",
    "            else :\n",
    "                norm_feature = normalization_features[index]\n",
    "                if not(norm_feature in batch.keys()) :\n",
    "                    batch = add_transformed_feature(batch, norm_feature)\n",
    "            batch['normalized_'+not_normalized_feature_name] = tensor_normalize(batch[not_normalized_feature_name], batch, \\\n",
    "                                                                                not_normalized_feature_name, norm_feature)\n",
    "        if feature.startswith('filtered_') :\n",
    "            not_filt_feature_name = feature.replace(\"filtered_\", \"\")\n",
    "            batch['filtered_'+not_filt_feature_name] = filter_with_convolution(batch[not_filt_feature_name], convolution_kernel_size=3)\n",
    "    # stack features from sample into channel (create channel dimension in tensor)\n",
    "    stacked_channels = torch.stack([cut_bords(batch[key], nb_of_border_pix) for key in features])\n",
    "    if (len(stacked_channels.shape) == 4): # 2d data case -> 4D cubes \n",
    "        transform = torch.permute(stacked_channels, (1,0,2,3)).to(torch.float32) #shape [N,C,H,W]\n",
    "    if (len(stacked_channels.shape) == 5): # 3d data case -> 5d cubes\n",
    "        transform = torch.permute(stacked_channels, (1,2,0,3,4)).to(torch.float32) #shape [N,L,C,H,W]\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5989ce5-d5e6-4b5e-93b1-f4fc25882aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transformed_feature(batch, missing_feature_name) :\n",
    "    if missing_feature_name.startswith('filtered_') :\n",
    "        not_filt_feature_name = missing_feature_name.replace(\"filtered_\", \"\")\n",
    "        batch['filtered_'+not_filt_feature_name] = filter_with_convolution(batch[not_filt_feature_name], convolution_kernel_size=3)\n",
    "    if missing_feature_name.startswith('sqrt_filtered_') :\n",
    "        feature_name = missing_feature_name.replace(\"sqrt_filtered_\", \"\")\n",
    "        batch['sqrt_filtered_'+feature_name] = torch.sqrt(filter_with_convolution(batch[feature_name], convolution_kernel_size=3))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85a96d77-1deb-4f02-8fff-9f59ce6b81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_restore_norm(tensor, batch, reference_feature, normalization_feature=None) :\n",
    "    if (len(tensor.shape) == 3) :\n",
    "        std = batch['std_'+reference_feature][:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,None,None]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        std = batch['std_'+reference_feature][:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,None,None]\n",
    "    if (len(tensor.shape) == 5) :\n",
    "        std = batch['std_'+reference_feature][:,:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,:, None,None]\n",
    "    if (normalization_feature is None) :\n",
    "        return tensor*std+mean\n",
    "    else : \n",
    "        return tensor*batch[normalization_feature]+mean\n",
    "\n",
    "def tensor_normalize(tensor, batch, reference_feature, normalization_feature=None) :\n",
    "    if (len(tensor.shape) == 3) :\n",
    "        std = batch['std_'+reference_feature][:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,None,None]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        std = batch['std_'+reference_feature][:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,None,None]\n",
    "    if (len(tensor.shape) == 5) :\n",
    "        std = batch['std_'+reference_feature][:,:,:,None,None]\n",
    "        mean = batch['mean_'+reference_feature][:,:,:, None,None]\n",
    "    if (normalization_feature is None) :\n",
    "        return (tensor-mean)/std\n",
    "    else : \n",
    "        return (tensor-mean)/batch[normalization_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51e3d393-0a09-4eee-9b6a-5c5ab76f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_with_convolution(tensor, convolution_kernel_size=3) :\n",
    "    if (len(tensor.shape) == 4) : #[N,L,H,W]\n",
    "        batch_len, nb_levels, height, width = tensor.shape\n",
    "        flatten_tensor = tensor.flatten(start_dim=0, end_dim=1)[:,None,:,:]\n",
    "    if (len(tensor.shape) == 3) : #[N,H,W]\n",
    "        flatten_tensor = tensor[:,None,:,:]\n",
    "        \n",
    "    weights = torch.ones(1,1,convolution_kernel_size,convolution_kernel_size).to(tensor.device) #matrix filled with ones for averaging\n",
    "    padding = torch.nn.ReplicationPad2d(convolution_kernel_size//2)  #pad borders with 1 row/column with the replicated values\n",
    "    padded_tensor= padding(flatten_tensor)\n",
    "    res = torch.nn.functional.conv2d(padded_tensor, weights, bias=None, stride=1, padding='valid', dilation=1, groups=1)\n",
    "    res = res[:,0,:,:]\n",
    "    if (len(tensor.shape) == 4) :\n",
    "        res = res.unflatten(dim=0, sizes=(batch_len, nb_levels))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0a560-2600-409a-be49-cfe767b7d50a",
   "metadata": {},
   "source": [
    "## PyTorch Lighning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7744d06-1873-4f38-918b-030e009b6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericPyLiModule(pl.LightningModule):\n",
    "    def __init__(self, torch_model, input_features, output_features, output_units, loss, optimizer, learning_rate, \\\n",
    "                 input_normalization_features=None, loss_normalization=False):\n",
    "        super().__init__()\n",
    "        self.torch_model = torch_model\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.output_units = output_units\n",
    "        self.input_normalization_features = input_normalization_features\n",
    "        self.loss_normalization = loss_normalization\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_normalization = loss_normalization\n",
    "        \n",
    "        ## initialization of weights\n",
    "        #torch_model.weight.data = torch.Tensor([1.0])\n",
    "\n",
    "        #construct list of names of features to be predicted\n",
    "        self.list_of_features_to_predict=list()\n",
    "        for i, feature in enumerate(self.output_features) :\n",
    "            self.list_of_features_to_predict.append(feature)\n",
    "            if feature.startswith('normalized_') :\n",
    "                # if model output is a normalized feature then compute also the non-normalized feature (for the diagnostics)\n",
    "                not_normalized_feature = feature.replace(\"normalized_\", \"\")\n",
    "                self.list_of_features_to_predict.append(not_normalized_feature)\n",
    "                \n",
    "        self.data_geometry = self.torch_model.data_geometry\n",
    "    \n",
    "    def common_step(self, batch, batch_idx) :\n",
    "        x = transform_and_stack_features(batch, self.input_features, self.torch_model.cut_border_pix_input, self.input_normalization_features)\n",
    "        y_true = transform_and_stack_features(batch, self.output_features, self.torch_model.cut_border_pix_output)\n",
    "        mask = cut_bords(batch['eroded_mask'], self.torch_model.cut_border_pix_output)\n",
    "\n",
    "        if (self.output_units is None) :\n",
    "            y_model = self.torch_model(x)\n",
    "        else :\n",
    "            y_units = transform_and_stack_features(batch, self.output_units, self.torch_model.cut_border_pix_output)\n",
    "            y_model = y_units*self.torch_model(x)\n",
    "        \n",
    "        logs = dict()\n",
    "        if (self.torch_model == 'lin_regr_model') :\n",
    "            first_layer_weights = list(self.torch_model.__dict__['_modules'].values())[0].weight # for logging\n",
    "            first_weight = np.array(first_layer_weights.cpu().detach().numpy()).flat[0]\n",
    "            logs['first_weight'] = np.array(first_layer_weights.cpu().detach().numpy()).flat[0]\n",
    "        \n",
    "        if (self.loss=='pressure_based_MSEloss') :\n",
    "            if (self.data_geometry != '3D') :\n",
    "                print('ERROR: pressure based loss is available only for 3D data')\n",
    "                return\n",
    "            index_of_temp_var_feature = self.output_features.index('temp_var')\n",
    "            pred_sigma = y_model[:, :, index_of_temp_var_feature, :, :]\n",
    "            target_sigma = y_true[:, :, index_of_temp_var_feature, :, :]\n",
    "            loss_pres_grad = pressure_based_MSEloss(batch, pred_sigma, target_sigma, \\\n",
    "                                                    self.torch_model.cut_border_pix_output, \\\n",
    "                                                    idx_level=100, normalization=True)\n",
    "            loss_val = evaluate_loss_with_mask('3D', torch.nn.functional.mse_loss, mask, y_model, y_true, \\\n",
    "                                       reduction='mean', normalization=True)\n",
    "            alpha=1.\n",
    "            loss_total = alpha*loss_pres_grad+loss_val\n",
    "            logs = logs | dict({'loss_train' : loss_total,\n",
    "                 'loss_pressure' : loss_pres_grad,\n",
    "                 'loss_value' : loss_val})\n",
    "        else :\n",
    "            loss_val = evaluate_loss_with_mask(self.data_geometry, self.loss, mask, y_model, y_true, \\\n",
    "                                               reduction='mean', normalization=self.loss_normalization)  \n",
    "            logs = logs | dict({'loss_train' : loss_val})\n",
    "        return logs\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        logs = self.common_step(batch, batch_idx)\n",
    "        self.log_dict(logs, on_step=False, on_epoch=True)\n",
    "        return logs['loss_train']\n",
    "\n",
    "    # validation logics (is evaluated during the training, but the data is not used to the optimization loop)\n",
    "    def validation_step(self, batch, batch_idx) :\n",
    "        logs = self.common_step(batch, batch_idx)\n",
    "        self.log_dict(dict({'loss_validation' : logs['loss_train']}), on_step=False, on_epoch=True)\n",
    "    \n",
    "    # gives model output in a form of a dictionary of batches of 2d fields\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx) :\n",
    "        x = transform_and_stack_features(batch, self.input_features, self.torch_model.cut_border_pix_input)\n",
    "        \n",
    "        output_tensor = self.torch_model(x)\n",
    "        if (self.data_geometry == '2D') :\n",
    "            batch_len, nb_of_channels, output_h, output_w = output_tensor.shape\n",
    "        if (self.data_geometry == '3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = output_tensor.shape\n",
    "\n",
    "        if not(self.output_units is None) : # if output of the model is dimensionless -> compute output with physical units\n",
    "            y_units = transform_and_stack_features(batch, self.output_units, self.torch_model.cut_border_pix_output)\n",
    "            output_tensor_units = output_tensor*y_units\n",
    "            \n",
    "        # construct the dictionary of the predicted features by decomposing the channels into dictionary entities\n",
    "        pred = dict()\n",
    "        if (self.data_geometry == '2D') :\n",
    "            channel_dim = 1\n",
    "        if (self.data_geometry == '3D') :\n",
    "            channel_dim = 2\n",
    "        for i, feature in enumerate(self.output_features) :\n",
    "            if (self.output_units is None) :\n",
    "                pred[feature] = output_tensor.select(dim=channel_dim, index=i)\n",
    "            else :\n",
    "                pred[feature+'_dimless'] = output_tensor.select(dim=channel_dim, index=i)\n",
    "                pred[feature] = output_tensor_units.select(dim=channel_dim, index=i)\n",
    "            # if some outputs are normalized then compute also result in the restored units (not normalized)\n",
    "            if feature.startswith('normalized_') :\n",
    "                not_normalized_feature_name = feature.replace(\"normalized_\", \"\")\n",
    "                pred[not_normalized_feature_name] = PyLiDataModule.tensor_restore_norm(pred[feature], batch, not_normalized_feature)\n",
    "        \n",
    "        # save the mask and masked outputs (use the eroded mask)\n",
    "        for i, feature in enumerate(self.list_of_features_to_predict) :\n",
    "            if (self.data_geometry == '2D') :\n",
    "                 pred['mask'] = batch['eroded_mask']\n",
    "            if (self.data_geometry == '3D') :\n",
    "                pred['mask'] = batch['eroded_mask'][:,None,:,:]\n",
    "            pred[feature+'_masked'] = expand_to_bords(pred[feature], self.torch_model.cut_border_pix_output)\n",
    "            pred[feature+'_masked'] = pred[feature+'_masked'].where(pred['mask'], torch.ones_like(pred[feature+'_masked'])*np.nan)\n",
    "        return pred \n",
    "    \n",
    "    # testing logic - to evaluate the model after training\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx) :\n",
    "        pred = self.predict_step(batch, batch_idx, dataloader_idx)\n",
    "        mask = cut_bords(batch['eroded_mask'], self.torch_model.cut_border_pix_output)\n",
    "        \n",
    "        test_dict = dict({'loss_val' : dict(), 'loss_grad' : dict(), 'corr_coef' : dict(), 'corr_coef_grad' : dict()})\n",
    "        dict_for_log = dict()\n",
    "        \n",
    "        # global metrics\n",
    "        for i, feature in enumerate(self.list_of_features_to_predict) :\n",
    "            truth = cut_bords(batch[feature], self.torch_model.cut_border_pix_output)\n",
    "            model_output = pred[feature]  # use unmasked prediction here, mask will be applied further on error tensor\n",
    "            \n",
    "            test_dict['loss_val'][feature] = evaluate_loss_with_mask(self.data_geometry, torch.nn.functional.mse_loss, \\\n",
    "                                                                                    mask, model_output, truth, \\\n",
    "                                                                                    reduction='mean', normalization=True)\n",
    "            test_dict['corr_coef'][feature] = torch.corrcoef(torch.vstack((torch.flatten(model_output).view(1,-1), \\\n",
    "                                                              torch.flatten(truth).view(1,-1))))[1,0]\n",
    "            # metrics on horizontal gradients\n",
    "            model_output_grad = finite_diffs_sqr_2d_map(model_output)\n",
    "            truth_grad = finite_diffs_sqr_2d_map(truth)\n",
    "            test_dict['loss_grad'][feature] = evaluate_loss_with_mask(self.data_geometry, torch.nn.functional.mse_loss, \\\n",
    "                                                                     mask[:,1:-1,1:-1], model_output_grad, truth_grad, \\\n",
    "                                                                      reduction='mean', normalization=True)\n",
    "            test_dict['corr_coef_grad'][feature] = torch.corrcoef(torch.vstack((torch.flatten(model_output_grad).view(1,-1), \\\n",
    "                                                              torch.flatten(truth_grad).view(1,-1))))[1,0]\n",
    "\n",
    "        # pressure at 100th level\n",
    "        if (self.data_geometry == '3D') :\n",
    "            idx_level = 100\n",
    "            true_temp_var = cut_bords(batch['temp_var'], self.torch_model.cut_border_pix_output)\n",
    "            model_temp_var = pred['temp_var']\n",
    "            test_dict['loss_val']['pressure_grad'] = pressure_based_MSEloss(batch, model_temp_var, true_temp_var, \\\n",
    "                                                    self.torch_model.cut_border_pix_output, \\\n",
    "                                                    idx_level=100, normalization=True)\n",
    "        for metrics in list(test_dict.keys()) : \n",
    "            for feature in list(test_dict[metrics].keys()) : \n",
    "                dict_for_log.update({(metrics+'_'+feature) : test_dict[metrics][feature]})\n",
    "        self.log_dict(dict_for_log)\n",
    "\n",
    "    def configure_optimizers(self) :\n",
    "        optimizer = self.optimizer(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685e749-f3b5-4140-9549-ba391d1902f6",
   "metadata": {},
   "source": [
    "## Torch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293381a9-a6f9-40df-a5fd-f31c67548398",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9428fc0-e221-4fc5-815e-37d30b9ba01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_regr_model(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.nb_of_input_features = nb_of_input_features\n",
    "        self.nb_of_output_features = nb_of_output_features\n",
    "        \n",
    "        self.cut_border_pix_output = None\n",
    "        self.cut_border_pix_input = None\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(self.nb_of_input_features, self.nb_of_output_features, bias=False)\n",
    "        \n",
    "        # initialization \n",
    "        self.lin1.weight.data = torch.Tensor([[0.1]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (self.data_geometry == '3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels, output_h, output_w = x.shape\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "            new_batch_len = batch_len*nb_of_levels\n",
    "        if (self.data_geometry == '2D') :\n",
    "            new_batch_len, nb_of_channels, output_h, output_w = x.shape\n",
    "            res = x \n",
    "        \n",
    "        # first split the input 4D torch tensor into individual pixels (equivalent to patches of size 1x1)\n",
    "        res = torch.nn.functional.unfold(res, kernel_size=1, dilation=1, padding=0, stride=1)\n",
    "        res = torch.permute(res, dims=(0,2,1))\n",
    "        res = torch.flatten(res, end_dim=1).to(torch.float32)\n",
    "        \n",
    "        # perform linear regression\n",
    "        res = self.lin1(res)\n",
    "        \n",
    "        # reshape the model output back to a 4D torch tensor\n",
    "        res = torch.permute(res.unflatten(dim=0, sizes=[new_batch_len,-1]),dims=(0,2,1))\n",
    "        res = torch.nn.functional.fold(res, output_size=(output_h,output_w), kernel_size=1, dilation=1, padding=0, stride=1)\n",
    "        \n",
    "        if (self.data_geometry == '3D') :\n",
    "            # unflatten the levels back\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fe844-bd44-405a-bbf1-b623c9413acf",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e0c83d9-b648-42bd-8a76-c5dcfe0fbf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features, input_patch_size, output_patch_size, \\\n",
    "                 activation_function = torch.nn.functional.relu, int_layer_width=50):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.input_patch_size = input_patch_size\n",
    "        self.output_patch_size = output_patch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.int_layer_width = int_layer_width\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(nb_of_input_features*input_patch_size**2, int_layer_width, bias=True)\n",
    "        self.lin2 = torch.nn.Linear(int_layer_width, int_layer_width, bias=True)\n",
    "        self.lin3 = torch.nn.Linear(int_layer_width, nb_of_output_features*output_patch_size**2, bias=True)\n",
    "        \n",
    "        self.cut_border_pix_output = self.input_patch_size//2 - self.output_patch_size//2\n",
    "        if (self.cut_border_pix_output < 1) :\n",
    "            self.cut_border_pix_output = None\n",
    "        self.cut_border_pix_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (self.data_geometry =='3D') :\n",
    "            batch_len, nb_of_levels, nb_of_channels = x.shape[0:3]\n",
    "            output_h = x.shape[3]-2*(self.cut_border_pix_output or 0)\n",
    "            output_w = x.shape[4]-2*(self.cut_border_pix_output or 0)\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "            new_batch_len = batch_len*nb_of_levels\n",
    "        if (self.data_geometry =='2D') :\n",
    "            new_batch_len, nb_of_channels = x.shape[0:2]\n",
    "            output_h = x.shape[2]-2*(self.cut_border_pix_output or 0)\n",
    "            output_w = x.shape[3]-2*(self.cut_border_pix_output or 0)\n",
    "            res = x\n",
    "        \n",
    "        # create patches of size 'input_patch_size' and join them into batches (zero padding - will remove border pixels)\n",
    "        res = torch.nn.functional.unfold(res, kernel_size=self.input_patch_size, dilation=1, padding=0, stride=1)\n",
    "        res = torch.permute(res, dims=(0,2,1))\n",
    "        res = torch.flatten(res, end_dim=1)\n",
    "        \n",
    "        # pass though the FCNN\n",
    "        res = self.lin1(res)\n",
    "        res = self.activation_function(res)\n",
    "        res = self.lin2(res)\n",
    "        res = self.activation_function(res)\n",
    "        res = self.lin3(res)\n",
    "        \n",
    "        # reshape the output patches back into a 4D torch tensor\n",
    "        res = res.unflatten(dim=0, sizes=(new_batch_len,-1))\n",
    "        res = torch.permute(res,dims=(0,2,1))\n",
    "        res = torch.nn.functional.fold(res, output_size=(output_h,output_w), \\\n",
    "                                       kernel_size=self.output_patch_size, dilation=1, padding=0, stride=1)\n",
    "        # compute the divider needed to get correct values in case of overlapping patches (will give mean over all overlapping patches)\n",
    "        mask_ones = torch.ones((1,1,output_h,output_w)).to(x.device)\n",
    "        divisor = torch.nn.functional.fold(torch.nn.functional.unfold(mask_ones, kernel_size=self.output_patch_size), \\\n",
    "                                           kernel_size=self.output_patch_size, output_size=(output_h,output_w))   \n",
    "        res = res/divisor.view(1,1,output_h,output_w)\n",
    "        \n",
    "        if (self.data_geometry =='3D') :\n",
    "            # unflatten the levels\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187ee78-26ad-4d0e-b6d5-305005316106",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74881638-7fc9-4f4b-aec0-32aa60ce0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, data_geometry, nb_of_input_features, nb_of_output_features, padding='same', padding_mode='replicate', \\\n",
    "                 kernel_size=3, int_layer_width=64, activation_function = torch.nn.functional.relu):\n",
    "        super().__init__()\n",
    "        self.data_geometry = data_geometry\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_mode = 'replicate'\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.cut_border_pix_input = None\n",
    "        if self.padding == 'same' :\n",
    "            self.cut_border_pix_output = self.cut_border_pix_input\n",
    "        if self.padding == 'valid' :\n",
    "            self.cut_border_pix_output = (self.cut_border_pix_input or 0) + self.kernel_size//2\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=nb_of_input_features, out_channels=int_layer_width, kernel_size=self.kernel_size, \\\n",
    "                                     padding=self.padding,  padding_mode=self.padding_mode) \n",
    "        self.conv2 = torch.nn.Conv2d(int_layer_width, int_layer_width, kernel_size=self.kernel_size, padding='same', padding_mode=self.padding_mode) \n",
    "        self.conv3 = torch.nn.Conv2d(int_layer_width, nb_of_output_features, kernel_size=self.kernel_size, padding='same', \\\n",
    "                                     padding_mode=self.padding_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_len = x.shape[0]\n",
    "        if (self.data_geometry == '3D') :\n",
    "            nb_of_levels = x.shape[1]\n",
    "            # deattach levels into batch entities by flattening\n",
    "            res = x.flatten(start_dim=0, end_dim=1) # shape [N',C,H,W]\n",
    "        else :\n",
    "            res = x\n",
    "        \n",
    "        res = self.conv1(res)\n",
    "        res = self.activation_function(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.activation_function(res)\n",
    "        res = self.conv3(res)\n",
    "        \n",
    "        if (self.data_geometry == '3D') :\n",
    "            # unflatten the levels\n",
    "            res = res.unflatten(dim=0, sizes=(batch_len, nb_of_levels))\n",
    "        return res       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d97b74-61d1-43d2-9d9e-3b825250c8b6",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c71bf7a-9728-4235-a85e-a6c03599113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, project) :\n",
    "    wandb_logger = WandbLogger(name=config['model_label']+'_'+config['version'], \\\n",
    "                               version=config['model_label']+'_'+config['version'],\\\n",
    "                               project=project, config=config, resume=False, log_model=False, offline=True)\n",
    "    \n",
    "    torch_model = eval(config['torch_model'])(**config['torch_model_params'])\n",
    "    pylight_module = GenericPyLiModule(torch_model, **config['module_params'])\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"loss_train\", save_last=True)    \n",
    "    early_stopping_callback = EarlyStopping(monitor=\"loss_train\", mode=\"min\")\n",
    "    \n",
    "    trainer = pl.Trainer(**config['training_params'], logger=wandb_logger, \n",
    "                     callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                     accelerator='gpu', devices=(1 if torch.cuda.is_available() else None))  \n",
    "    trainer.fit(model = pylight_module, datamodule=eval(config['datamodule']))\n",
    "    #tests\n",
    "    test_datamodule = eval(config['datamodule'])\n",
    "    test_datamodule.setup(stage='test')\n",
    "    trainer.test(model = pylight_module, datamodule=test_datamodule)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de8768-4f7f-4c10-af64-d618b8b4230e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88828ed-5e5b-452b-aab8-da0947b420d1",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76db4448-d28a-4270-bb64-6de61cd4e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_BUCKET = os.environ['PERSISTENT_BUCKET'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f449a2-50f9-494d-bf4c-77f853392e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict=[dict() for i in range(2)]\n",
    "data_dict[0] = dict({'region' : '1', 'season' : 'fma', 'label' : 'GULFSTR FMA'})\n",
    "data_dict[1] = dict({'region' : '1', 'season' : 'aso', 'label' : 'GULFSTR ASO'})\n",
    "# data_dict[2] = dict({'region' : '2', 'season' : 'fma', 'label' : 'MIDATL FMA'})\n",
    "# data_dict[3] = dict({'region' : '2', 'season' : 'aso', 'label' : 'MIDATL ASO'})\n",
    "# data_dict[4] = dict({'region' : '3', 'season' : 'fma', 'label' : 'WESTMED FMA'})\n",
    "# data_dict[5] = dict({'region' : '3', 'season' : 'aso', 'label' : 'WESTMED ASO'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7aa0e4b6-6c0d-4cce-95c0-223506fa3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "height = 45\n",
    "width = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34bb9812-17c9-41ee-83d6-46221e925bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 1.37 s, total: 9.6 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_to_add_to_sample = ['temp', 'saline', 'temp_var', 'rho_ct_ct', 'diff_temp_sqr']\n",
    "auxiliary_features = ['z_l', 'f', 'e1t', 'e2t']\n",
    "all_data_3D = PyLiDataModule(data_dict, '3D', features_to_add_to_sample, auxiliary_features, height, width, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b42b115b-6c02-4dd4-8e95-4059c37b917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 777 ms, sys: 110 ms, total: 888 ms\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_to_add_to_sample = ['temp', 'saline', 'temp_var', 'rho_ct_ct', 'diff_temp_sqr']\n",
    "auxiliary_features = ['e1t', 'e2t']\n",
    "all_data_2D = PyLiDataModule(data_dict, '2D', features_to_add_to_sample, auxiliary_features, height, width, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04341a50-38c4-4c47-ab3b-8636903857a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_2D.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ca341b0-0750-427b-9617-4d6ada9fb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_2D.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc491c31-1b9b-4c61-84c9-0f2c2e35d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = all_data_2D.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90a2a134-3304-4edb-8713-d9cf64de1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_di = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93260680-4519-472b-bdd5-2b0086d0742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(test_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7848419a-e5ee-42d9-ad7c-3b7b0f33b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mask', 'eroded_mask', 'mean_temp', 'std_temp', 'temp', 'mean_saline', 'std_saline', 'saline', 'mean_temp_var', 'std_temp_var', 'temp_var', 'mean_rho_ct_ct', 'std_rho_ct_ct', 'rho_ct_ct', 'mean_diff_temp_sqr', 'std_diff_temp_sqr', 'diff_temp_sqr', 'e1t', 'e2t'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb222872-8f0c-49b1-a4bb-609b91832511",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dcb94fb-1f4c-44fc-a746-fd938e31bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62aaf6ff-5b20-46a9-833c-a3d7326f87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df30436e-5ce6-4094-bbe2-b7b8287fcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/LinReg_HuberLoss2D_nn/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | torch_model | lin_regr_model | 1     \n",
      "-----------------------------------------------\n",
      "1         Trainable params\n",
      "0         Non-trainable params\n",
      "1         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d7afd9bb28401a9892b27ec661cfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae9dcdf50654042a5743c02c9f125dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃<span style=\"font-weight: bold\">       DataLoader 2        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  corr_coef_grad_temp_var  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7017481923103333     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.764130175113678     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7076631188392639     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    corr_coef_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7137855887413025     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7573876976966858     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7262170314788818     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_grad_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     446.687744140625      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     245.015869140625      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     344.1112365722656     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     loss_val_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.599957466125488     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    11.112597465515137     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.860700607299805     </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 2       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m corr_coef_grad_temp_var \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7017481923103333    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.764130175113678    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7076631188392639    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   corr_coef_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7137855887413025    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7573876976966858    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7262170314788818    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_grad_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    446.687744140625     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    245.015869140625     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    344.1112365722656    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    loss_val_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.599957466125488    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.112597465515137    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.860700607299805    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_validation</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>0.70175</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>0.76413</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>0.70766</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>0.71379</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>0.75739</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>0.72622</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>446.68774</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>245.01587</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>344.11124</td></tr><tr><td>loss_train</td><td>0.08128</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>10.59996</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>11.1126</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>10.8607</td></tr><tr><td>loss_validation</td><td>0.07411</td></tr><tr><td>trainer/global_step</td><td>1302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230406_152130-LinReg_HuberLoss2D_nn<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230406_152130-LinReg_HuberLoss2D_nn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = dict({'model_label' : 'LinReg',\n",
    "                'version' : 'HuberLoss2D_nn',\n",
    "                'torch_model' : 'lin_regr_model',\n",
    "                'datamodule' : 'all_data_2D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '2D',\\\n",
    "                                            'nb_of_input_features' : 1, \\\n",
    "                                            'nb_of_output_features' : 1}),\n",
    "                'module_params' : dict({'input_features'  : ['diff_temp_sqr'],\n",
    "                                        'output_features'  : ['temp_var'],\n",
    "                                        'output_units' : None,\n",
    "                                        'loss' : torch.nn.functional.huber_loss,\n",
    "                                        'optimizer' : torch.optim.SGD,\n",
    "                                        'learning_rate' : 1e-3,\n",
    "                                        'loss_normalization' : False}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cebc87e-7278-4bdf-a1f1-a65f2b48d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/FCNN_FCNN_3D/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type | Params\n",
      "-------------------------------------\n",
      "0 | torch_model | FCNN | 6.0 K \n",
      "-------------------------------------\n",
      "6.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f371663d3f484b8776429acdcd4278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42b896fecfd4f7b801373223e9234e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃<span style=\"font-weight: bold\">       DataLoader 2        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   corr_coef_grad_saline   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.15368036925792694    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11728319525718689    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13457462191581726    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  corr_coef_grad_temp_var  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.04198182374238968    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.047130513936281204   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -0.0440402589738369    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     corr_coef_saline      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -0.8817467093467712    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -0.7573999166488647    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -0.7970536947250366    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    corr_coef_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.054502617567777634   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.022027989849448204   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.03802590072154999    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     loss_grad_saline      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     6.52175235748291      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     7.584216594696045     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    7.0673418045043945     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_grad_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    20.370820999145508     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     14.01479434967041     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    17.106916427612305     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  loss_val_pressure_grad   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     28.72718868170139     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     28.57512214328806     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     28.64910045927292     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      loss_val_saline      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    35.387821197509766     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     34.04207992553711     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     34.6967658996582      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     loss_val_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2096844911575317     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1668298244476318     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1876780986785889     </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 2       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  corr_coef_grad_saline  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15368036925792694   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11728319525718689   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13457462191581726   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m corr_coef_grad_temp_var \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.04198182374238968   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.047130513936281204  \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -0.0440402589738369   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    corr_coef_saline     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -0.8817467093467712   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -0.7573999166488647   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -0.7970536947250366   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   corr_coef_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.054502617567777634  \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.022027989849448204  \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.03802590072154999   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    loss_grad_saline     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    6.52175235748291     \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    7.584216594696045    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   7.0673418045043945    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_grad_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   20.370820999145508    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    14.01479434967041    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   17.106916427612305    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m loss_val_pressure_grad  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    28.72718868170139    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    28.57512214328806    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    28.64910045927292    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     loss_val_saline     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   35.387821197509766    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    34.04207992553711    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    34.6967658996582     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    loss_val_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2096844911575317    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1668298244476318    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1876780986785889    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_saline/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_saline/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_grad_saline/dataloader_idx_2</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>corr_coef_saline/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_saline/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_saline/dataloader_idx_2</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>loss_grad_saline/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_saline/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_grad_saline/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_pressure</td><td>█▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_val_saline/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_saline/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_saline/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_validation</td><td>█▁</td></tr><tr><td>loss_value</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_saline/dataloader_idx_0</td><td>0.15368</td></tr><tr><td>corr_coef_grad_saline/dataloader_idx_1</td><td>0.11728</td></tr><tr><td>corr_coef_grad_saline/dataloader_idx_2</td><td>0.13457</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>-0.04198</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>-0.04713</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>-0.04404</td></tr><tr><td>corr_coef_saline/dataloader_idx_0</td><td>-0.88175</td></tr><tr><td>corr_coef_saline/dataloader_idx_1</td><td>-0.7574</td></tr><tr><td>corr_coef_saline/dataloader_idx_2</td><td>-0.79705</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>-0.0545</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>-0.02203</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>-0.03803</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>loss_grad_saline/dataloader_idx_0</td><td>6.52175</td></tr><tr><td>loss_grad_saline/dataloader_idx_1</td><td>7.58422</td></tr><tr><td>loss_grad_saline/dataloader_idx_2</td><td>7.06734</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>20.37082</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>14.01479</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>17.10692</td></tr><tr><td>loss_pressure</td><td>29.77675</td></tr><tr><td>loss_train</td><td>56.02093</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_0</td><td>28.72719</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_1</td><td>28.57512</td></tr><tr><td>loss_val_pressure_grad/dataloader_idx_2</td><td>28.6491</td></tr><tr><td>loss_val_saline/dataloader_idx_0</td><td>35.38782</td></tr><tr><td>loss_val_saline/dataloader_idx_1</td><td>34.04208</td></tr><tr><td>loss_val_saline/dataloader_idx_2</td><td>34.69677</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>1.20968</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>1.16683</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>1.18768</td></tr><tr><td>loss_validation</td><td>46.6395</td></tr><tr><td>loss_value</td><td>26.24418</td></tr><tr><td>trainer/global_step</td><td>52</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230406_152221-FCNN_FCNN_3D<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230406_152221-FCNN_FCNN_3D/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = dict({'model_label' : 'FCNN',\n",
    "                'version' : 'FCNN_3D',\n",
    "                'torch_model' : 'FCNN',\n",
    "                'datamodule' : 'all_data_3D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '3D',\\\n",
    "                                            'nb_of_input_features' : 2, \\\n",
    "                                            'nb_of_output_features' : 2, \\\n",
    "                                            'input_patch_size' : 5,\n",
    "                                            'output_patch_size' : 3, \n",
    "                                            'int_layer_width' : 50}),\n",
    "                'module_params' : dict({'input_features'  : ['temp', 'saline'],\n",
    "                                        'output_features'  : ['temp_var', 'saline'],\n",
    "                                        'output_units' : None,\n",
    "                                        'input_normalization_features' : None,\n",
    "                                        'loss_normalization' : True,\n",
    "                                        'loss' : 'pressure_based_MSEloss',\n",
    "                                        'optimizer' : torch.optim.Adam,\n",
    "                                        'learning_rate' : 1e-3,}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57671b09-bf3d-4333-8094-348aa016ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'torch_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['torch_model'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory ./debug/FCNN_FCNN_2D/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type | Params\n",
      "-------------------------------------\n",
      "0 | torch_model | FCNN | 5.6 K \n",
      "-------------------------------------\n",
      "5.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad45903dd8845a0875835d4761f6599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e276cd53eacf470ea2f8d4b97a64a66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃<span style=\"font-weight: bold\">       DataLoader 2        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  corr_coef_grad_temp_var  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.014783068560063839    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.013041188940405846    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.020163636654615402    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    corr_coef_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.06908224523067474    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.05446900054812431    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.06456063687801361    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    loss_grad_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     118.8787841796875     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     187.5087127685547     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     153.7860565185547     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     loss_val_temp_var     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    6.4648027420043945     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     66.72342681884766     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     37.11418533325195     </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 2       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m corr_coef_grad_temp_var \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.014783068560063839   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.013041188940405846   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.020163636654615402   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   corr_coef_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.06908224523067474   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.05446900054812431   \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.06456063687801361   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   loss_grad_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    118.8787841796875    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    187.5087127685547    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    153.7860565185547    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    loss_val_temp_var    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   6.4648027420043945    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    66.72342681884766    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    37.11418533325195    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▅▅█</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_train</td><td>█▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>▁</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>▁</td></tr><tr><td>loss_validation</td><td>▁█</td></tr><tr><td>trainer/global_step</td><td>▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_coef_grad_temp_var/dataloader_idx_0</td><td>0.01478</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_1</td><td>0.01304</td></tr><tr><td>corr_coef_grad_temp_var/dataloader_idx_2</td><td>0.02016</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_0</td><td>-0.06908</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_1</td><td>-0.05447</td></tr><tr><td>corr_coef_temp_var/dataloader_idx_2</td><td>-0.06456</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_0</td><td>118.87878</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_1</td><td>187.50871</td></tr><tr><td>loss_grad_temp_var/dataloader_idx_2</td><td>153.78606</td></tr><tr><td>loss_train</td><td>0.68483</td></tr><tr><td>loss_val_temp_var/dataloader_idx_0</td><td>6.4648</td></tr><tr><td>loss_val_temp_var/dataloader_idx_1</td><td>66.72343</td></tr><tr><td>loss_val_temp_var/dataloader_idx_2</td><td>37.11419</td></tr><tr><td>loss_validation</td><td>1.67565</td></tr><tr><td>trainer/global_step</td><td>1302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ./wandb/offline-run-20230406_152243-FCNN_FCNN_2D<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20230406_152243-FCNN_FCNN_2D/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config7=dict({'model_label' : 'FCNN',\n",
    "                'version' : 'FCNN_2D',\n",
    "                'torch_model' : 'FCNN',\n",
    "                'datamodule' : 'all_data_2D',\n",
    "                'torch_model_params' : dict({'data_geometry' : '2D',\\\n",
    "                                            'nb_of_input_features' : 2, \\\n",
    "                                            'nb_of_output_features' : 1, \\\n",
    "                                            'input_patch_size' : 5,\n",
    "                                            'output_patch_size' : 3, \n",
    "                                            'int_layer_width' : 50,\n",
    "                                            'activation_function' : torch.nn.functional.elu}),\n",
    "                'module_params' : dict({'input_features'  : ['temp', 'saline'],\n",
    "                                        'output_features'  : ['temp_var'],\n",
    "                                        'output_units' : ['filtered_diff_temp_sqr'],\n",
    "                                        'input_normalization_features' : None,\n",
    "                                        'loss_normalization' : True,\n",
    "                                        'loss' : torch.nn.functional.huber_loss,\n",
    "                                        'optimizer' : torch.optim.Adam,\n",
    "                                        'learning_rate' : 1e-3,}),\n",
    "                'training_params' : dict({'max_epochs' : 2,\n",
    "                                          'limit_train_batches' : 1.0})\n",
    "               })\n",
    "run_experiment(config7, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41771c55-c85a-4129-9fd4-529659f9b12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8a613-56f4-4774-98b9-a8d96e26a773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
